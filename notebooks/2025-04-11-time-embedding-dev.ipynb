{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12caac1e",
   "metadata": {},
   "source": [
    "# Enhanced Embedding Layer for PolarBERT\n",
    "\n",
    "**Goal:** Improve model performance and physical relevance by replacing the simple linear projection of features with learned embeddings, particularly for time and charge, while correctly handling relative time and masking.\n",
    "\n",
    "**Branch:** `eat/time-embedding`\n",
    "\n",
    "**Key Changes & Implementation Steps (GPU-based):**\n",
    "\n",
    "1.  **Input Processing (Inside `EnhancedIceCubeEmbedding.forward`):**\n",
    "    * Receive batch `(x, l)` from the dataloader (where `x` contains normalized float features).\n",
    "    * Perform all subsequent transformations on the GPU using PyTorch tensor operations.\n",
    "\n",
    "2.  **Time Feature (`x[:, :, 0]`):**\n",
    "    * **Relative Time Calculation:**\n",
    "        * Invert normalization: `t_float = t_norm * 3e4 + 1e4`.\n",
    "        * Calculate `t_min` per event, carefully ignoring padding values (mask with `inf` before `torch.min`).\n",
    "        * Calculate relative time: `t_relative = t_float - t_min`.\n",
    "    * **Integer Conversion & Clipping:**\n",
    "        * Round `t_relative` to the nearest integer (`torch.round().long()`).\n",
    "        * Clip the integer time to a predefined maximum (`MAX_TIME_VOCAB`, e.g., 51996) using `torch.clamp()`.\n",
    "    * **Padding Index:** Use `torch.where()` to replace values in originally padded positions with a dedicated `TIME_PADDING_IDX` (e.g., 51997).\n",
    "    * **Embedding:** Use an `nn.Embedding(time_vocab_size, time_embed_dim, padding_idx=TIME_PADDING_IDX)` layer with the resulting integer indices. `time_vocab_size` needs to accommodate `MAX_TIME_VOCAB`, `PAD`, `MASK`, etc. (e.g., 52000).\n",
    "\n",
    "3.  **Charge Feature (`x[:, :, 1]`):**\n",
    "    * **TODO:**\n",
    "        * Define a quantization strategy (e.g., log binning, quantile binning based on data analysis).\n",
    "        * Invert normalization: `log10_charge = charge_norm * 3.0`, then `charge = 10**log10_charge`.\n",
    "        * Apply quantization function to get integer bin indices.\n",
    "        * Handle padding.\n",
    "        * Use an `nn.Embedding` layer for charge bins.\n",
    "\n",
    "4.  **Auxiliary Feature (`x[:, :, 2]`):**\n",
    "    * Can use a small `nn.Embedding` (e.g., 2-3 embeddings for True/False/Padding) or keep a simple linear projection. Needs careful handling of the input value (`aux - 0.5`).\n",
    "    * Crucially, the *original* value of this feature is needed to determine which pulses *not* to mask during the masking step.\n",
    "\n",
    "5.  **DOM ID Feature (`x[:, :, 3]`):**\n",
    "    * Input is `sensor_id + 1`, padding is `0`.\n",
    "    * Use `nn.Embedding(dom_vocab_size, dom_embed_dim, padding_idx=0)`. `dom_vocab_size` includes actual IDs + PAD + MASK token (e.g., 5160 + 2).\n",
    "\n",
    "6.  **Combining Embeddings:**\n",
    "    * Concatenate the embeddings for Time, DOM, Charge, and Auxiliary.\n",
    "    * Use a final `nn.Linear` layer to project the concatenated vector to the model's target `embedding_dim`.\n",
    "\n",
    "7.  **Special Tokens & Masking:**\n",
    "    * **CLS Token:** Prepend a learned `cls_embedding` parameter (`nn.Parameter`) to the sequence *after* processing pulse embeddings.\n",
    "    * **Masking:**\n",
    "        * If `masking=True`, calculate a boolean `output_mask` based on `mask_prob`, ensuring auxiliary pulses (`x[:, :, 2] == 0.5`) and padded pulses are *not* masked.\n",
    "        * Define a learned `mask_token_embed` parameter (`nn.Parameter` of size `embedding_dim`).\n",
    "        * Use `torch.where(output_mask.unsqueeze(-1), mask_token_embed, projected_embeddings)` to replace the embeddings of masked positions with the learned mask embedding *after* the projection layer.\n",
    "    * **Padding Mask:** Generate the final `final_padding_mask` (for the transformer attention) including the prepended CLS token (which is never padded).\n",
    "\n",
    "8.  **Output:** The `forward` method should return `full_embedding`, `final_padding_mask`, and `output_mask` (if masking is enabled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dabbfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polarbert.pretraining import get_dataloaders, load_and_process_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db025e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_and_process_config('/groups/pheno/inar/PolarBERT/configs/polarbert_IT.yaml')\n",
    "config['data']['train_events'] = 100_000\n",
    "config['training']['per_device_batch_size'] = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "793fd16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = get_dataloaders(config, dataset_type='kaggle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8b44654",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb48ce95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO improve docs to specify what are the collumns in the data!\n",
    "\n",
    "# T_evt[:len(selected_idx), 0] = (time[selected_idx] - 1e4) / 3e4\n",
    "# T_evt[:len(selected_idx), 1] = np.log10(charge[selected_idx]) / 3.0\n",
    "# T_evt[:len(selected_idx), 2] = auxiliary[selected_idx] - 0.5 # aux = True is BAD, so 0.5 is bad\n",
    "# T_evt[:len(selected_idx), 3] = sensor_id[selected_idx] + 1  # +1 is needed since we use 0 for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b6eb0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x, l), (y, c) = batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f764a92",
   "metadata": {},
   "source": [
    "Notice that we pad everything by zero!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "212a8a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6019.2871,  6103.5156,  6169.4336,  6367.1875,  6376.3428,  6425.7812,\n",
      "         6520.9961,  6605.2246,  6989.7461,  7495.1172,  7745.9717,  8077.3926,\n",
      "         8168.9453,  8253.1738,  8359.3750,  9252.0137,  9328.0029,  9365.9971,\n",
      "         9516.1436,  9547.9580,  9658.0508,  9720.0771,  9878.0059, 10086.9746,\n",
      "        10100.9941, 10102.0244, 10128.9746, 10144.9971, 10184.0215, 10319.9766,\n",
      "        10567.1689, 10846.8633, 11046.4473, 11126.0986, 11320.1904, 12532.3486,\n",
      "        12817.9932, 13537.5977, 13958.7402, 14094.2383, 14101.5625, 14156.4941,\n",
      "        15240.4785, 15280.7617, 16134.0332, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 6019,  6104,  6169,  6367,  6376,  6426,  6521,  6605,  6990,  7495,\n",
       "         7746,  8077,  8169,  8253,  8359,  9252,  9328,  9366,  9516,  9548,\n",
       "         9658,  9720,  9878, 10087, 10101, 10102, 10129, 10145, 10184, 10320,\n",
       "        10567, 10847, 11046, 11126, 11320, 12532, 12818, 13538, 13959, 14094,\n",
       "        14102, 14156, 15240, 15281, 16134, 10000, 10000, 10000, 10000, 10000,\n",
       "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
       "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
       "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
       "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
       "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
       "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
       "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
       "        10000, 10000, 10000, 10000, 10000, 10000, 10000], dtype=torch.int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "times = x[0][:,0] * 3e4 + 1e4\n",
    "print(times)\n",
    "# Round times to integers using torch\n",
    "times_rounded = torch.round(times).int()\n",
    "times_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d0d174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np # Only needed for np.inf if torch.inf is not available\n",
    "\n",
    "# --- Define Constants ---\n",
    "# Based on proposed vocab_size = 52000 for time\n",
    "MAX_TIME_VOCAB = 51996   # Max relative time value allowed (0 to 51996 ns)\n",
    "TIME_PADDING_IDX = 51997 # Index for padded time positions\n",
    "TIME_MASK_IDX = 51998    # Reserved index for MASK token in time vocab\n",
    "# TIME_UNUSED_IDX = 51999 # Reserved index\n",
    "\n",
    "DOM_PADDING_IDX = 0      # Padding index used for DOM IDs in input data\n",
    "DOM_MASK_IDX = 5161      # Reserved index for MASK token in DOM vocab (5160 real DOMs)\n",
    "DOM_VOCAB_SIZE = 5160 + 2 # 5160 DOMs + PAD + MASK\n",
    "\n",
    "class EnhancedIceCubeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding layer for IceCube data using learned embeddings for time (relative),\n",
    "    DOM ID, and potentially charge/auxiliary features. Performs transformations\n",
    "    on-the-fly on the GPU within the forward pass. Handles masking correctly.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, masking=False):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.masking = masking\n",
    "        self.mask_prob = config['training']['mask_prob'] if masking else 0.0\n",
    "\n",
    "        embedding_dim = config['model']['embedding_dim']\n",
    "        # Get sub-embedding dimensions from config, with defaults\n",
    "        dom_embed_dim = config['model'].get('dom_embed_dim', 64)\n",
    "        time_embed_dim = config['model'].get('time_embed_dim', 128)\n",
    "        charge_embed_dim = config['model'].get('charge_embed_dim', 32) # Placeholder size\n",
    "        aux_embed_dim = config['model'].get('aux_embed_dim', 32)       # Placeholder size\n",
    "\n",
    "        time_vocab_size = 52000 # Includes PAD, MASK etc.\n",
    "\n",
    "        # --- Embedding Layers ---\n",
    "        self.dom_embedding = nn.Embedding(DOM_VOCAB_SIZE, dom_embed_dim, padding_idx=DOM_PADDING_IDX)\n",
    "        self.time_embedding = nn.Embedding(time_vocab_size, time_embed_dim, padding_idx=TIME_PADDING_IDX)\n",
    "\n",
    "        # --- TODO: Define Charge Embedding ---\n",
    "        # Example: Placeholder - simple projection for now\n",
    "        charge_input_dim = 1 # log10(charge)/3.0 from input feature 1\n",
    "        self.charge_proj = nn.Linear(charge_input_dim, charge_embed_dim)\n",
    "        # Replace above with quantization + nn.Embedding when ready\n",
    "\n",
    "        # --- TODO: Define Auxiliary Embedding ---\n",
    "        # Example: Placeholder - simple projection for now\n",
    "        aux_input_dim = 1 # aux - 0.5 from input feature 2\n",
    "        self.aux_proj = nn.Linear(aux_input_dim, aux_embed_dim)\n",
    "        # Could also use nn.Embedding(3, aux_embed_dim) if mapping -0.5, 0.5 to indices\n",
    "\n",
    "        # --- Mask Token Parameter ---\n",
    "        if self.masking:\n",
    "             # Learned embedding vector for the [MASK] token\n",
    "             self.mask_token_embed = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
    "\n",
    "        # --- Projection Layer ---\n",
    "        # Calculates total dimension from individual embeddings/projections\n",
    "        total_sub_embed_dim = dom_embed_dim + time_embed_dim + charge_embed_dim + aux_embed_dim\n",
    "        self.projection = nn.Linear(total_sub_embed_dim, embedding_dim)\n",
    "\n",
    "        # --- CLS Token ---\n",
    "        self.cls_embedding = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
    "\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        x, l = input_batch # x shape: (batch, seq_len, features), l shape: (batch,)\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        # --- 1. Get Original Padding Mask (based on DOM ID) ---\n",
    "        # Input x[:, :, 3] has 0 for padding, and sensor_id+1 otherwise.\n",
    "        padding_mask = (x[:, :, 3] == DOM_PADDING_IDX) # Shape: (batch, seq_len)\n",
    "\n",
    "        # --- 2. Process Time Feature ---\n",
    "        time_normalized = x[:, :, 0]\n",
    "        # Invert normalization\n",
    "        time_float_approx = time_normalized * 3e4 + 1e4\n",
    "        # Calculate t_min, ignoring padding\n",
    "        time_float_masked_for_min = torch.where(padding_mask, torch.full_like(time_float_approx, float('inf')), time_float_approx)\n",
    "        t_min_per_event = torch.min(time_float_masked_for_min, dim=1, keepdim=True)[0]\n",
    "        t_min_per_event = torch.where(torch.isinf(t_min_per_event), torch.zeros_like(t_min_per_event), t_min_per_event) # Handle all-padding events\n",
    "        # Calculate relative time\n",
    "        time_relative_float = time_float_approx - t_min_per_event\n",
    "        # Round, cast to long, clip\n",
    "        time_relative_int = torch.round(time_relative_float).long()\n",
    "        time_relative_int_clipped = torch.clamp(time_relative_int, min=0, max=MAX_TIME_VOCAB)\n",
    "        # Apply padding index\n",
    "        time_indices = torch.where(padding_mask, torch.full_like(time_relative_int_clipped, TIME_PADDING_IDX), time_relative_int_clipped)\n",
    "        # Get time embeddings\n",
    "        time_embeds = self.time_embedding(time_indices) # Shape: (batch, seq_len, time_embed_dim)\n",
    "\n",
    "        # --- 3. Process DOM ID Feature ---\n",
    "        dom_ids = x[:, :, 3].long() # Already prepared: 0 for pad, ID+1 otherwise\n",
    "        dom_embeds = self.dom_embedding(dom_ids) # Shape: (batch, seq_len, dom_embed_dim)\n",
    "\n",
    "        # --- 4. Process Charge Feature (Placeholder) ---\n",
    "        charge_normalized = x[:, :, 1].unsqueeze(-1) # Keep feature dim\n",
    "        # TODO: Invert normalization, quantize, use nn.Embedding\n",
    "        charge_embeds = self.charge_proj(charge_normalized) # Using projection for now\n",
    "\n",
    "        # --- 5. Process Auxiliary Feature (Placeholder) ---\n",
    "        aux_normalized = x[:, :, 2].unsqueeze(-1) # Keep feature dim\n",
    "        # TODO: Implement nn.Embedding lookup based on value (-0.5 or 0.5)\n",
    "        aux_embeds = self.aux_proj(aux_normalized) # Using projection for now\n",
    "\n",
    "        # --- 6. Combine Embeddings & Project ---\n",
    "        combined_sub_embeds = torch.cat([dom_embeds, time_embeds, charge_embeds, aux_embeds], dim=2)\n",
    "        projected_embeds = self.projection(combined_sub_embeds) # Shape: (batch, seq_len, embedding_dim)\n",
    "\n",
    "        # --- 7. Apply Masking (if enabled) ---\n",
    "        output_mask = None # Mask tensor to be returned for loss calculation\n",
    "        if self.masking:\n",
    "            # Identify non-auxiliary pulses (original aux feature value was -0.5)\n",
    "            is_non_auxiliary = (x[:, :, 2] == -0.5)\n",
    "            # Generate random mask based on probability\n",
    "            random_mask = torch.rand(is_non_auxiliary.shape, device=device) < self.mask_prob\n",
    "\n",
    "            # Combine conditions: Mask only non-auxiliary, non-padded positions that are randomly selected\n",
    "            output_mask = is_non_auxiliary & random_mask & ~padding_mask # Shape: (batch, seq_len)\n",
    "\n",
    "            # Replace the projected embeddings with the learned mask token embedding\n",
    "            projected_embeds = torch.where(\n",
    "                output_mask.unsqueeze(-1), # Expand mask shape for broadcasting\n",
    "                self.mask_token_embed.expand(batch_size, seq_len, -1), # Expand mask token\n",
    "                projected_embeds # Keep original embedding if not masked\n",
    "            )\n",
    "\n",
    "        # --- 8. Prepend CLS Token ---\n",
    "        cls_token_expanded = self.cls_embedding.expand(batch_size, -1, -1)\n",
    "        full_embedding = torch.cat([cls_token_expanded, projected_embeds], dim=1) # Shape: (batch, seq_len+1, embedding_dim)\n",
    "\n",
    "        # --- 9. Create Final Padding Mask (for Transformer Attention) ---\n",
    "        # Includes position for CLS token (never padded)\n",
    "        final_padding_mask = torch.cat([\n",
    "             torch.zeros(batch_size, 1, dtype=torch.bool, device=device), # CLS padding (False)\n",
    "             padding_mask # Original sequence padding mask\n",
    "        ], dim=1) # Shape: (batch, seq_len+1)\n",
    "\n",
    "        # --- 10. Return necessary outputs ---\n",
    "        # output_mask is the boolean mask indicating which input tokens were masked (needed for loss)\n",
    "        # final_padding_mask indicates padding for the attention mechanism (includes CLS)\n",
    "        return full_embedding, final_padding_mask, output_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bda4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polarbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
