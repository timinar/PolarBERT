{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bde38507",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fcacd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polarbert.config import PolarBertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a337119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration from: /groups/pheno/inar/PolarBERT/configs/polarbert_new.yaml\n",
      "Validating configuration...\n",
      "Configuration validation passed (with potential warnings).\n",
      "Calculating runtime training parameters...\n",
      "  Logical Batch Size: 4096\n",
      "  Max Per Device Batch Size: 4096\n",
      "  Calculated Per Device Batch Size: 4096\n",
      "  Gradient Accumulation Steps: 1\n",
      "  Effective Batch Size: 4096\n",
      "  Steps per Epoch (logical): 10000\n",
      "  Total Steps: 200000\n",
      "  Overriding pct_start based on warm_up_steps: 0.2000 -> 0.0050\n",
      "\n",
      "--- Calculated Runtime Params ---\n",
      "Total Steps: 200000\n",
      "Grad Accum Steps: 1\n",
      "Effective Batch Size: 4096\n",
      "Final pct_start for Scheduler: 0.0050\n",
      "\n",
      "--- Accessing Config ---\n",
      "Model Name: polarbert_time_embed\n",
      "Time Embedding Dim: 128\n",
      "Optimizer Beta1: 0.9\n",
      "Checkpoint Dir: checkpoints\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "config_file = \"/groups/pheno/inar/PolarBERT/configs/polarbert_new.yaml\"\n",
    "\n",
    "try:\n",
    "    config = PolarBertConfig.from_yaml(config_file)\n",
    "\n",
    "    # --- Calculate Runtime Params (example, needs actual train_loader length) ---\n",
    "    # Replace '10000' with len(train_loader) from your script\n",
    "    # Note: len(dataloader) with IterableDataset might be tricky, often needs manual setting.\n",
    "    # Let's assume you know the number of batches per epoch for the dataloader.\n",
    "    num_batches_per_epoch = 10000 # Placeholder! Calculate this properly.\n",
    "    config.calculate_runtime_params(num_batches_per_epoch)\n",
    "    print(\"\\n--- Calculated Runtime Params ---\")\n",
    "    print(f\"Total Steps: {config.training.total_steps}\")\n",
    "    print(f\"Grad Accum Steps: {config.training.gradient_accumulation_steps}\")\n",
    "    print(f\"Effective Batch Size: {config.training.effective_batch_size}\")\n",
    "    print(f\"Final pct_start for Scheduler: {config.training.pct_start:.4f}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Accessing Config ---\")\n",
    "    print(f\"Model Name: {config.model.model_name}\")\n",
    "    print(f\"Time Embedding Dim: {config.model.embedding.time_embedding_dim}\")\n",
    "    print(f\"Optimizer Beta1: {config.training.adam_beta1}\")\n",
    "    print(f\"Checkpoint Dir: {config.training.checkpoint.dirpath}\")\n",
    "\n",
    "    # Example saving\n",
    "    # config.save_yaml('saved_config_copy.yaml')\n",
    "\n",
    "    # Example loading from checkpoint dir\n",
    "    # cp_path = 'checkpoints/your_model_run/epoch=01-step=1000.ckpt'\n",
    "    # config_from_cp = PolarBertConfig.from_checkpoint(cp_path)\n",
    "    # print(f\"\\nLoaded config from checkpoint dir. Project: {config_from_cp.training.logging.project}\")\n",
    "\n",
    "except (FileNotFoundError, ValueError, yaml.YAMLError, TypeError) as e:\n",
    "     print(f\"\\nError loading/validating/calculating config: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22a2f270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'max_per_device_batch_size': 4096,\n",
      "          'num_workers': 1,\n",
      "          'persistent_workers': True,\n",
      "          'pin_memory': False,\n",
      "          'train_dir': '/groups/pheno/inar/icecube_kaggle/memmaped_100M_127',\n",
      "          'train_events': 100000000,\n",
      "          'val_dir': '/groups/pheno/inar/icecube_kaggle/memmaped_eval_1M_127',\n",
      "          'val_events': 200000},\n",
      " 'model': {'embedding': {'aux_embedding_dim': 4,\n",
      "                         'charge_embedding_dim': 16,\n",
      "                         'charge_vocab_size': 128,\n",
      "                         'dom_embedding_dim': 108,\n",
      "                         'dom_vocab_size': 5162,\n",
      "                         'embedding_dim': 256,\n",
      "                         'embedding_projection': False,\n",
      "                         'masking_charges': False,\n",
      "                         'masking_doms': True,\n",
      "                         'masking_prob': 0.25,\n",
      "                         'masking_times': False,\n",
      "                         'time_embedding_dim': 128,\n",
      "                         'time_vocab_size': 52000},\n",
      "           'embedding_dim': 256,\n",
      "           'ffd_type': 'SwiGLU',\n",
      "           'hidden_size': 1024,\n",
      "           'lambda_charge': 1.0,\n",
      "           'model_name': 'polarbert_time_embed',\n",
      "           'num_heads': 8,\n",
      "           'num_layers': 8},\n",
      " 'training': {'adam_beta1': 0.9,\n",
      "              'adam_beta2': 0.95,\n",
      "              'adam_eps': 1e-08,\n",
      "              'amsgrad': False,\n",
      "              'checkpoint': {'dirpath': 'checkpoints',\n",
      "                             'mode': 'min',\n",
      "                             'monitor': 'val/full_loss',\n",
      "                             'save_final': True,\n",
      "                             'save_last': True,\n",
      "                             'save_top_k': 1},\n",
      "              'div_factor': 25.0,\n",
      "              'final_div_factor': 10000.0,\n",
      "              'gpus': 1,\n",
      "              'gradient_clip_val': 1.0,\n",
      "              'logging': {'project': 'PolarBERT-time-embed'},\n",
      "              'logical_batch_size': 4096,\n",
      "              'lr_scheduler': 'onecycle',\n",
      "              'max_epochs': 20,\n",
      "              'max_lr': 0.0003,\n",
      "              'optimizer': 'AdamW',\n",
      "              'pct_start': 0.005,\n",
      "              'precision': '16-mixed',\n",
      "              'val_check_interval': 0.5,\n",
      "              'warm_up_steps': 1000,\n",
      "              'weight_decay': 0.1}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "config_dict = config.to_dict()\n",
    "pprint(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345650bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "583e2719",
   "metadata": {},
   "source": [
    "config parameters\n",
    "Embedding:\n",
    "- time_embedding_dim: 128\n",
    "- dom_embedding_dim: 111 # with aux (1) and charge (16) will be 128\n",
    "- charge_embedding_dim: 16\n",
    "- time_vocab_size: 52000 # includes, padding, maksing, and overflow tokens\n",
    "- dom_vocab_size: 5162 # includes padding and masking tokens\n",
    "- charge_num_bins: 32 # includes padding and masking tokens\n",
    "- masking_doms: True\n",
    "- masking_times: True\n",
    "- masking_charges: True\n",
    "- masking_prob: 0.25 \n",
    "Model\n",
    "- num_layers: 8\n",
    "- num_heads: 4 # assert sum emb dim is divisible by num_heads\n",
    "- hidden_size: 1024\n",
    "- ffd_type: \"SwiGLU\" # or \"MLP\"\n",
    "- use_rope: False\n",
    "- use_positional_embedding: False # assert use_rope is False\n",
    "\n",
    "data:\n",
    "  max_per_device_batch_size: 1024 # Maximum batch size that fits on the GPU\n",
    "  train_dir: '/path/to/train/data'\n",
    "  val_dir: '/path/to/val/data'\n",
    "  train_events: 10000000\n",
    "  val_events: 200000\n",
    "  pin_memory: false\n",
    "  num_workers: 1 # Should remain 1 when using an IterableDataset\n",
    "  persistent_workers: true\n",
    "\n",
    "training:\n",
    "  max_epochs: 1\n",
    "  logical_batch_size: 200 # Batch size used for training (will use gradient accumulation if necessary)\n",
    "  val_check_interval: 0.1\n",
    "  gpus: 1\n",
    "  precision: \"16-mixed\"\n",
    "  gradient_clip_val: 2.0\n",
    "  max_lr: 3e-3\n",
    "  optimizer: \"AdamW\"\n",
    "  optimier_kwargs:\n",
    "    betas: [0.9, 0.95]\n",
    "    eps: 1e-8\n",
    "    weight_decay: 0.1\n",
    "  scheduler: 'onecycle'\n",
    "  warm_up_steps: null  # Number of steps for learning rate warm-up\n",
    "  pct_start: 0.2       # Percentage of training for warm-up when warm_up_steps is not provided\n",
    "  div_factor: 25.0\n",
    "  final_div_factor: 1e4\n",
    "  \n",
    "logging:\n",
    "  project: 'PolarBERT'\n",
    "  checkpoint:\n",
    "    dirpath: 'checkpoints'\n",
    "    save_top_k: 1\n",
    "    monitor: 'val/full_loss'\n",
    "    mode: 'min'\n",
    "    save_last: true\n",
    "    save_final: true\n",
    "  \n",
    "\n",
    "\n",
    "MAX_TIME_VOCAB = 51996   # Max relative time value allowed (0 to 51996 ns)\n",
    "TIME_PADDING_IDX = 51997 # Index for padded time positions\n",
    "TIME_MASK_IDX = 51998    # Reserved index for MASK token in time vocab\n",
    "# TIME_UNUSED_IDX = 51999 # Reserved index\n",
    "\n",
    "DOM_PADDING_IDX = 0      # Padding index used for DOM IDs in input data\n",
    "DOM_MASK_IDX = 5161      # Reserved index for MASK token in DOM vocab (5160 real DOMs)\n",
    "DOM_VOCAB_SIZE = 5160 + 2 # 5160 DOMs + PAD + MASK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ce500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np # Only needed for np.inf if torch.inf is not available\n",
    "\n",
    "# --- Define Constants (Using PAD=0, MASK=LAST_IDX scheme) ---\n",
    "\n",
    "PAD_IDX = 0\n",
    "\n",
    "# Time Vocab\n",
    "TIME_VOCAB_SIZE = 52000 # Total size: 0=PAD, 1..51998=Data(0..51997ns), 51999=MASK\n",
    "TIME_MASK_IDX = TIME_VOCAB_SIZE - 1 # 51999\n",
    "# Max duration corresponds to index TIME_MASK_IDX - 1\n",
    "MAX_TIME_DURATION = TIME_MASK_IDX - 1 # 51998 ns (Indices 1 to 51998 cover this range)\n",
    "\n",
    "# DOM Vocab\n",
    "DOM_VOCAB_SIZE = 5162 # 0=PAD, 1..5160=Data(IDs 1..5160), 5161=MASK\n",
    "DOM_MASK_IDX = DOM_VOCAB_SIZE - 1 # 5161\n",
    "\n",
    "# Charge Vocab (Example: 32 bins -> indices 0..31 for data)\n",
    "CHARGE_NUM_BINS = 32 # Number of actual data bins\n",
    "CHARGE_VOCAB_SIZE = CHARGE_NUM_BINS + 2 # 0=PAD, 1..32=Data(bins 0..31), 33=MASK\n",
    "CHARGE_MASK_IDX = CHARGE_VOCAB_SIZE - 1 # 33\n",
    "\n",
    "# Auxiliary Vocab (Example: 2 categories -> indices 0, 1 for data)\n",
    "AUX_NUM_CATS = 2\n",
    "AUX_VOCAB_SIZE = AUX_NUM_CATS + 2 # 0=PAD, 1=Data(False), 2=Data(True), 3=MASK\n",
    "AUX_MASK_IDX = AUX_VOCAB_SIZE - 1 # 3\n",
    "\n",
    "\n",
    "class EnhancedIceCubeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding layer using index replacement for masking specific features.\n",
    "    Indices: 0=[PAD], LAST_IDX=[MASK]. Assumes sub-embedding dims sum\n",
    "    to the final model embedding dimension (no projection layer).\n",
    "    \"\"\"\n",
    "    def __init__(self, config, masking=False):\n",
    "        super().__init__()\n",
    "        self.config = config # Store the main ExperimentConfig object\n",
    "        self.masking = masking\n",
    "        self.mask_prob = config.training.mask_prob if masking else 0.0\n",
    "\n",
    "        embedding_cfg = config.embedding\n",
    "        model_cfg = config.model\n",
    "\n",
    "        # --- Embedding Layers (All use padding_idx=0) ---\n",
    "        self.dom_embedding = nn.Embedding(\n",
    "            DOM_VOCAB_SIZE, # Use constant calculated above\n",
    "            embedding_cfg.dom_embedding_dim,\n",
    "            padding_idx=PAD_IDX\n",
    "        )\n",
    "        self.time_embedding = nn.Embedding(\n",
    "            TIME_VOCAB_SIZE,\n",
    "            embedding_cfg.time_embedding_dim,\n",
    "            padding_idx=PAD_IDX\n",
    "        )\n",
    "        # --- TODO: Define Charge Embedding ---\n",
    "        self.charge_embedding = nn.Embedding(\n",
    "             CHARGE_VOCAB_SIZE,\n",
    "             embedding_cfg.charge_embedding_dim,\n",
    "             padding_idx=PAD_IDX\n",
    "        )\n",
    "        # --- TODO: Define Auxiliary Embedding ---\n",
    "        self.aux_embedding = nn.Embedding(\n",
    "             AUX_VOCAB_SIZE,\n",
    "             embedding_cfg.aux_embedding_dim,\n",
    "             padding_idx=PAD_IDX\n",
    "        )\n",
    "\n",
    "        # --- Calculate Total Dimension & Validate ---\n",
    "        total_sub_embed_dim = (\n",
    "            embedding_cfg.dom_embedding_dim +\n",
    "            embedding_cfg.time_embedding_dim +\n",
    "            embedding_cfg.charge_embedding_dim +\n",
    "            embedding_cfg.aux_embedding_dim\n",
    "        )\n",
    "        # Final dimension expected by transformer blocks\n",
    "        self.embedding_dim = embedding_cfg.embedding_dim\n",
    "        if total_sub_embed_dim != self.embedding_dim:\n",
    "             raise ValueError(\n",
    "                 f\"Sum of sub-embedding dimensions ({total_sub_embed_dim}) does not match \"\n",
    "                 f\"target embedding.embedding_dim ({self.embedding_dim}). \"\n",
    "                 f\"Add a projection layer or adjust dimensions.\"\n",
    "             )\n",
    "\n",
    "        # --- CLS Token ---\n",
    "        # CLS embedding needs to match the final embedding dimension\n",
    "        self.cls_embedding = nn.Parameter(torch.randn(1, 1, self.embedding_dim))\n",
    "\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        x, l = input_batch # x shape: (batch, seq_len, features), l shape: (batch,)\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        device = x.device\n",
    "        embedding_cfg = self.config.embedding # Use embedding config\n",
    "\n",
    "        # --- 1. Get Original Padding Mask (based on DOM ID feature input) ---\n",
    "        padding_mask = (x[:, :, 3] == PAD_IDX) # Shape: (batch, seq_len)\n",
    "\n",
    "        # --- 2. Process Time Feature to Indices ---\n",
    "        time_normalized = x[:, :, 0]\n",
    "        time_float_approx = time_normalized * 3e4 + 1e4\n",
    "        time_float_masked_for_min = torch.where(padding_mask, torch.full_like(time_float_approx, float('inf')), time_float_approx)\n",
    "        t_min_per_event = torch.min(time_float_masked_for_min, dim=1, keepdim=True)[0]\n",
    "        t_min_per_event = torch.where(torch.isinf(t_min_per_event), torch.zeros_like(t_min_per_event), t_min_per_event)\n",
    "        #TODO first long, then subtract!\n",
    "        time_relative_float = time_float_approx - t_min_per_event\n",
    "        time_relative_int = torch.round(time_relative_float).long()\n",
    "        # Clip relative time *before* shifting indices\n",
    "        #TODO overlflow\n",
    "        time_relative_int_clipped = torch.clamp(time_relative_int, min=0, max=MAX_TIME_DURATION)\n",
    "        # Shift indices (data starts from index 1) and handle padding (map padding to PAD_IDX=0)\n",
    "        time_indices = torch.where(padding_mask, PAD_IDX, time_relative_int_clipped + 1) # Data indices: 1 to MAX_TIME_DURATION+1\n",
    "\n",
    "        # --- 3. Process DOM ID Feature to Indices ---\n",
    "        dom_ids_orig = x[:, :, 3].long() # 0 for pad, ID+1 for data (1 to 5161 effective range)\n",
    "        # No shift needed if data is already 1-based. Padding is already 0.\n",
    "        # Just need to ensure max value doesn't exceed vocab size allows (it shouldn't)\n",
    "        dom_indices = dom_ids_orig # Data indices: 1 to 5160\n",
    "\n",
    "        # --- 4. Process Charge Feature to Indices (Placeholder) ---\n",
    "        charge_normalized = x[:, :, 1]\n",
    "        # TODO: Implement: charge_float = 10**(charge_normalized * 3.0)\n",
    "        # TODO: Implement: charge_bin_idx = self.quantize_charge(charge_float) # Output indices 0 to CHARGE_NUM_BINS-1\n",
    "        # TODO: Implement: charge_bin_idx_clipped = torch.clamp(charge_bin_idx, 0, CHARGE_NUM_BINS-1)\n",
    "        # Placeholder: creating dummy indices\n",
    "        charge_bin_idx_clipped = torch.zeros_like(dom_indices) # Replace with real calculation\n",
    "        # Shift indices (data starts from 1) and handle padding\n",
    "        charge_indices = torch.where(padding_mask, PAD_IDX, charge_bin_idx_clipped + 1) # Data indices: 1 to CHARGE_NUM_BINS\n",
    "\n",
    "        # --- 5. Process Auxiliary Feature to Indices (Placeholder) ---\n",
    "        aux_normalized = x[:, :, 2] # -0.5 for False, 0.5 for True\n",
    "        # Map -0.5 -> 0, 0.5 -> 1 (example base indices for the 2 categories)\n",
    "        aux_base_idx = torch.round(aux_normalized + 0.5).long() # Maps -0.5 to 0, 0.5 to 1\n",
    "        aux_base_idx_clipped = torch.clamp(aux_base_idx, 0, AUX_NUM_CATS - 1)\n",
    "        # Shift indices (data starts from 1) and handle padding\n",
    "        aux_indices = torch.where(padding_mask, PAD_IDX, aux_base_idx_clipped + 1) # Data indices: 1 (False), 2 (True)\n",
    "\n",
    "        # --- 6. Apply Masking (Index Replacement) ---\n",
    "        output_mask = None # Boolean mask indicating which positions were chosen for masking\n",
    "        if self.masking:\n",
    "            is_non_auxiliary = (x[:, :, 2] == -0.5)\n",
    "            random_mask = torch.rand(is_non_auxiliary.shape, device=device) < self.mask_prob\n",
    "            output_mask = is_non_auxiliary & random_mask & ~padding_mask # Shape: (batch, seq_len)\n",
    "\n",
    "            # Conditionally replace indices with feature-specific MASK_IDX\n",
    "            if embedding_cfg.masking_doms:\n",
    "                dom_indices = torch.where(output_mask, DOM_MASK_IDX, dom_indices)\n",
    "            if embedding_cfg.masking_times:\n",
    "                time_indices = torch.where(output_mask, TIME_MASK_IDX, time_indices)\n",
    "            if embedding_cfg.masking_charges:\n",
    "                charge_indices = torch.where(output_mask, CHARGE_MASK_IDX, charge_indices)\n",
    "            # if embedding_cfg.masking_aux: # Add aux masking flag if needed\n",
    "            #    aux_indices = torch.where(output_mask, AUX_MASK_IDX, aux_indices)\n",
    "\n",
    "\n",
    "        # --- 7. Embedding Lookups ---\n",
    "        dom_embeds = self.dom_embedding(dom_indices)\n",
    "        time_embeds = self.time_embedding(time_indices)\n",
    "        charge_embeds = self.charge_embedding(charge_indices) # Using placeholder indices for now\n",
    "        aux_embeds = self.aux_embedding(aux_indices)       # Using placeholder indices for now\n",
    "\n",
    "        # --- 8. Combine Embeddings (No Projection Layer) ---\n",
    "        combined_embeds = torch.cat([dom_embeds, time_embeds, charge_embeds, aux_embeds], dim=2)\n",
    "        # Shape: (batch, seq_len, embedding_dim) - Assumes sum matches target dim\n",
    "\n",
    "        # --- 9. Prepend CLS Token ---\n",
    "        cls_token_expanded = self.cls_embedding.expand(batch_size, -1, -1)\n",
    "        full_embedding = torch.cat([cls_token_expanded, combined_embeds], dim=1) # Shape: (batch, seq_len+1, embedding_dim)\n",
    "\n",
    "        # --- 10. Create Final Padding Mask (for Transformer Attention) ---\n",
    "        final_padding_mask = torch.cat([\n",
    "             torch.zeros(batch_size, 1, dtype=torch.bool, device=device), # CLS padding (False)\n",
    "             padding_mask # Original sequence padding mask\n",
    "        ], dim=1) # Shape: (batch, seq_len+1)\n",
    "\n",
    "        # --- 11. Return ---\n",
    "        return full_embedding, final_padding_mask, output_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacb7ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3194af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6ea6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12caac1e",
   "metadata": {},
   "source": [
    "# Enhanced Embedding Layer for PolarBERT\n",
    "\n",
    "**Goal:** Improve model performance and physical relevance by replacing the simple linear projection of features with learned embeddings, particularly for time and charge, while correctly handling relative time and masking.\n",
    "\n",
    "**Branch:** `eat/time-embedding`\n",
    "\n",
    "**Key Changes & Implementation Steps (GPU-based):**\n",
    "\n",
    "1.  **Input Processing (Inside `EnhancedIceCubeEmbedding.forward`):**\n",
    "    * Receive batch `(x, l)` from the dataloader (where `x` contains normalized float features).\n",
    "    * Perform all subsequent transformations on the GPU using PyTorch tensor operations.\n",
    "\n",
    "2.  **Time Feature (`x[:, :, 0]`):**\n",
    "    * **Relative Time Calculation:**\n",
    "        * Invert normalization: `t_float = t_norm * 3e4 + 1e4`.\n",
    "        * Calculate `t_min` per event, carefully ignoring padding values (mask with `inf` before `torch.min`).\n",
    "        * Calculate relative time: `t_relative = t_float - t_min`.\n",
    "    * **Integer Conversion & Clipping:**\n",
    "        * Round `t_relative` to the nearest integer (`torch.round().long()`).\n",
    "        * Clip the integer time to a predefined maximum (`MAX_TIME_VOCAB`, e.g., 51996) using `torch.clamp()`.\n",
    "    * **Padding Index:** Use `torch.where()` to replace values in originally padded positions with a dedicated `TIME_PADDING_IDX` (e.g., 51997).\n",
    "    * **Embedding:** Use an `nn.Embedding(time_vocab_size, time_embed_dim, padding_idx=TIME_PADDING_IDX)` layer with the resulting integer indices. `time_vocab_size` needs to accommodate `MAX_TIME_VOCAB`, `PAD`, `MASK`, etc. (e.g., 52000).\n",
    "\n",
    "3.  **Charge Feature (`x[:, :, 1]`):**\n",
    "    * **TODO:**\n",
    "        * Define a quantization strategy (e.g., log binning, quantile binning based on data analysis).\n",
    "        * Invert normalization: `log10_charge = charge_norm * 3.0`, then `charge = 10**log10_charge`.\n",
    "        * Apply quantization function to get integer bin indices.\n",
    "        * Handle padding.\n",
    "        * Use an `nn.Embedding` layer for charge bins.\n",
    "\n",
    "4.  **Auxiliary Feature (`x[:, :, 2]`):**\n",
    "    * Can use a small `nn.Embedding` (e.g., 2-3 embeddings for True/False/Padding) or keep a simple linear projection. Needs careful handling of the input value (`aux - 0.5`).\n",
    "    * Crucially, the *original* value of this feature is needed to determine which pulses *not* to mask during the masking step.\n",
    "\n",
    "5.  **DOM ID Feature (`x[:, :, 3]`):**\n",
    "    * Input is `sensor_id + 1`, padding is `0`.\n",
    "    * Use `nn.Embedding(dom_vocab_size, dom_embed_dim, padding_idx=0)`. `dom_vocab_size` includes actual IDs + PAD + MASK token (e.g., 5160 + 2).\n",
    "\n",
    "6.  **Combining Embeddings:**\n",
    "    * Concatenate the embeddings for Time, DOM, Charge, and Auxiliary.\n",
    "    * Use a final `nn.Linear` layer to project the concatenated vector to the model's target `embedding_dim`.\n",
    "\n",
    "7.  **Special Tokens & Masking:**\n",
    "    * **CLS Token:** Prepend a learned `cls_embedding` parameter (`nn.Parameter`) to the sequence *after* processing pulse embeddings.\n",
    "    * **Masking:**\n",
    "        * If `masking=True`, calculate a boolean `output_mask` based on `mask_prob`, ensuring auxiliary pulses (`x[:, :, 2] == 0.5`) and padded pulses are *not* masked.\n",
    "        * Define a learned `mask_token_embed` parameter (`nn.Parameter` of size `embedding_dim`).\n",
    "        * Use `torch.where(output_mask.unsqueeze(-1), mask_token_embed, projected_embeddings)` to replace the embeddings of masked positions with the learned mask embedding *after* the projection layer.\n",
    "    * **Padding Mask:** Generate the final `final_padding_mask` (for the transformer attention) including the prepended CLS token (which is never padded).\n",
    "\n",
    "8.  **Output:** The `forward` method should return `full_embedding`, `final_padding_mask`, and `output_mask` (if masking is enabled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b24e84d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2973ffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(10, 4, padding_idx=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb8323b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2103,  1.2373,  1.5075,  1.8231]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb(torch.tensor([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c755f11d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dabbfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polarbert.pretraining import get_dataloaders, load_and_process_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db025e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_and_process_config('/groups/pheno/inar/PolarBERT/configs/polarbert_IT.yaml')\n",
    "config['data']['train_events'] = 100_000\n",
    "config['training']['per_device_batch_size'] = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "793fd16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = get_dataloaders(config, dataset_type='kaggle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8b44654",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb48ce95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO improve docs to specify what are the collumns in the data!\n",
    "\n",
    "# T_evt[:len(selected_idx), 0] = (time[selected_idx] - 1e4) / 3e4\n",
    "# T_evt[:len(selected_idx), 1] = np.log10(charge[selected_idx]) / 3.0\n",
    "# T_evt[:len(selected_idx), 2] = auxiliary[selected_idx] - 0.5 # aux = True is BAD, so 0.5 is bad\n",
    "# T_evt[:len(selected_idx), 3] = sensor_id[selected_idx] + 1  # +1 is needed since we use 0 for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b6eb0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x, l), (y, c) = batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f764a92",
   "metadata": {},
   "source": [
    "Notice that we pad everything by zero!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "212a8a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6019.2871,  6103.5156,  6169.4336,  6367.1875,  6376.3428,  6425.7812,\n",
      "         6520.9961,  6605.2246,  6989.7461,  7495.1172,  7745.9717,  8077.3926,\n",
      "         8168.9453,  8253.1738,  8359.3750,  9252.0137,  9328.0029,  9365.9971,\n",
      "         9516.1436,  9547.9580,  9658.0508,  9720.0771,  9878.0059, 10086.9746,\n",
      "        10100.9941, 10102.0244, 10128.9746, 10144.9971, 10184.0215, 10319.9766,\n",
      "        10567.1689, 10846.8633, 11046.4473, 11126.0986, 11320.1904, 12532.3486,\n",
      "        12817.9932, 13537.5977, 13958.7402, 14094.2383, 14101.5625, 14156.4941,\n",
      "        15240.4785, 15280.7617, 16134.0332, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000, 10000.0000,\n",
      "        10000.0000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 6019,  6104,  6169,  6367,  6376,  6426,  6521,  6605,  6990,  7495,\n",
       "         7746,  8077,  8169,  8253,  8359,  9252,  9328,  9366,  9516,  9548,\n",
       "         9658,  9720,  9878, 10087, 10101, 10102, 10129, 10145, 10184, 10320,\n",
       "        10567, 10847, 11046, 11126, 11320, 12532, 12818, 13538, 13959, 14094,\n",
       "        14102, 14156, 15240, 15281, 16134, 10000, 10000, 10000, 10000, 10000,\n",
       "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
       "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
       "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
       "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
       "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
       "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
       "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
       "        10000, 10000, 10000, 10000, 10000, 10000, 10000], dtype=torch.int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "times = x[0][:,0] * 3e4 + 1e4\n",
    "print(times)\n",
    "# Round times to integers using torch\n",
    "times_rounded = torch.round(times).int()\n",
    "times_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d0d174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np # Only needed for np.inf if torch.inf is not available\n",
    "\n",
    "# --- Define Constants ---\n",
    "# Based on proposed vocab_size = 52000 for time\n",
    "MAX_TIME_VOCAB = 51996   # Max relative time value allowed (0 to 51996 ns)\n",
    "TIME_PADDING_IDX = 51997 # Index for padded time positions\n",
    "TIME_MASK_IDX = 51998    # Reserved index for MASK token in time vocab\n",
    "# TIME_UNUSED_IDX = 51999 # Reserved index\n",
    "\n",
    "DOM_PADDING_IDX = 0      # Padding index used for DOM IDs in input data\n",
    "DOM_MASK_IDX = 5161      # Reserved index for MASK token in DOM vocab (5160 real DOMs)\n",
    "DOM_VOCAB_SIZE = 5160 + 2 # 5160 DOMs + PAD + MASK\n",
    "\n",
    "class EnhancedIceCubeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding layer for IceCube data using learned embeddings for time (relative),\n",
    "    DOM ID, and potentially charge/auxiliary features. Performs transformations\n",
    "    on-the-fly on the GPU within the forward pass. Handles masking correctly.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, masking=False):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.masking = masking\n",
    "        self.mask_prob = config['training']['mask_prob'] if masking else 0.0\n",
    "\n",
    "        embedding_dim = config['model']['embedding_dim']\n",
    "        # Get sub-embedding dimensions from config, with defaults\n",
    "        dom_embed_dim = config['model'].get('dom_embed_dim', 64)\n",
    "        time_embed_dim = config['model'].get('time_embed_dim', 128)\n",
    "        charge_embed_dim = config['model'].get('charge_embed_dim', 32) # Placeholder size\n",
    "        aux_embed_dim = config['model'].get('aux_embed_dim', 32)       # Placeholder size\n",
    "\n",
    "        time_vocab_size = 52000 # Includes PAD, MASK etc.\n",
    "\n",
    "        # --- Embedding Layers ---\n",
    "        self.dom_embedding = nn.Embedding(DOM_VOCAB_SIZE, dom_embed_dim, padding_idx=DOM_PADDING_IDX)\n",
    "        self.time_embedding = nn.Embedding(time_vocab_size, time_embed_dim, padding_idx=TIME_PADDING_IDX)\n",
    "\n",
    "        # --- TODO: Define Charge Embedding ---\n",
    "        # Example: Placeholder - simple projection for now\n",
    "        charge_input_dim = 1 # log10(charge)/3.0 from input feature 1\n",
    "        self.charge_proj = nn.Linear(charge_input_dim, charge_embed_dim)\n",
    "        # Replace above with quantization + nn.Embedding when ready\n",
    "\n",
    "        # --- TODO: Define Auxiliary Embedding ---\n",
    "        # Example: Placeholder - simple projection for now\n",
    "        aux_input_dim = 1 # aux - 0.5 from input feature 2\n",
    "        self.aux_proj = nn.Linear(aux_input_dim, aux_embed_dim)\n",
    "        # Could also use nn.Embedding(3, aux_embed_dim) if mapping -0.5, 0.5 to indices\n",
    "\n",
    "        # --- Mask Token Parameter ---\n",
    "        if self.masking:\n",
    "             # Learned embedding vector for the [MASK] token\n",
    "             self.mask_token_embed = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
    "\n",
    "        # --- Projection Layer ---\n",
    "        # Calculates total dimension from individual embeddings/projections\n",
    "        total_sub_embed_dim = dom_embed_dim + time_embed_dim + charge_embed_dim + aux_embed_dim\n",
    "        self.projection = nn.Linear(total_sub_embed_dim, embedding_dim)\n",
    "\n",
    "        # --- CLS Token ---\n",
    "        self.cls_embedding = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
    "\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        x, l = input_batch # x shape: (batch, seq_len, features), l shape: (batch,)\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        # --- 1. Get Original Padding Mask (based on DOM ID) ---\n",
    "        # Input x[:, :, 3] has 0 for padding, and sensor_id+1 otherwise.\n",
    "        padding_mask = (x[:, :, 3] == DOM_PADDING_IDX) # Shape: (batch, seq_len)\n",
    "\n",
    "        # --- 2. Process Time Feature ---\n",
    "        time_normalized = x[:, :, 0]\n",
    "        # Invert normalization\n",
    "        time_float_approx = time_normalized * 3e4 + 1e4\n",
    "        # Calculate t_min, ignoring padding\n",
    "        time_float_masked_for_min = torch.where(padding_mask, torch.full_like(time_float_approx, float('inf')), time_float_approx)\n",
    "        t_min_per_event = torch.min(time_float_masked_for_min, dim=1, keepdim=True)[0]\n",
    "        t_min_per_event = torch.where(torch.isinf(t_min_per_event), torch.zeros_like(t_min_per_event), t_min_per_event) # Handle all-padding events\n",
    "        # Calculate relative time\n",
    "        time_relative_float = time_float_approx - t_min_per_event\n",
    "        # Round, cast to long, clip\n",
    "        time_relative_int = torch.round(time_relative_float).long()\n",
    "        time_relative_int_clipped = torch.clamp(time_relative_int, min=0, max=MAX_TIME_VOCAB)\n",
    "        # Apply padding index\n",
    "        time_indices = torch.where(padding_mask, torch.full_like(time_relative_int_clipped, TIME_PADDING_IDX), time_relative_int_clipped)\n",
    "        # Get time embeddings\n",
    "        time_embeds = self.time_embedding(time_indices) # Shape: (batch, seq_len, time_embed_dim)\n",
    "\n",
    "        # --- 3. Process DOM ID Feature ---\n",
    "        dom_ids = x[:, :, 3].long() # Already prepared: 0 for pad, ID+1 otherwise\n",
    "        dom_embeds = self.dom_embedding(dom_ids) # Shape: (batch, seq_len, dom_embed_dim)\n",
    "\n",
    "        # --- 4. Process Charge Feature (Placeholder) ---\n",
    "        charge_normalized = x[:, :, 1].unsqueeze(-1) # Keep feature dim\n",
    "        # TODO: Invert normalization, quantize, use nn.Embedding\n",
    "        charge_embeds = self.charge_proj(charge_normalized) # Using projection for now\n",
    "\n",
    "        # --- 5. Process Auxiliary Feature (Placeholder) ---\n",
    "        aux_normalized = x[:, :, 2].unsqueeze(-1) # Keep feature dim\n",
    "        # TODO: Implement nn.Embedding lookup based on value (-0.5 or 0.5)\n",
    "        aux_embeds = self.aux_proj(aux_normalized) # Using projection for now\n",
    "\n",
    "        # --- 6. Combine Embeddings & Project ---\n",
    "        combined_sub_embeds = torch.cat([dom_embeds, time_embeds, charge_embeds, aux_embeds], dim=2)\n",
    "        projected_embeds = self.projection(combined_sub_embeds) # Shape: (batch, seq_len, embedding_dim)\n",
    "\n",
    "        # --- 7. Apply Masking (if enabled) ---\n",
    "        output_mask = None # Mask tensor to be returned for loss calculation\n",
    "        if self.masking:\n",
    "            # Identify non-auxiliary pulses (original aux feature value was -0.5)\n",
    "            is_non_auxiliary = (x[:, :, 2] == -0.5)\n",
    "            # Generate random mask based on probability\n",
    "            random_mask = torch.rand(is_non_auxiliary.shape, device=device) < self.mask_prob\n",
    "\n",
    "            # Combine conditions: Mask only non-auxiliary, non-padded positions that are randomly selected\n",
    "            output_mask = is_non_auxiliary & random_mask & ~padding_mask # Shape: (batch, seq_len)\n",
    "\n",
    "            # Replace the projected embeddings with the learned mask token embedding\n",
    "            projected_embeds = torch.where(\n",
    "                output_mask.unsqueeze(-1), # Expand mask shape for broadcasting\n",
    "                self.mask_token_embed.expand(batch_size, seq_len, -1), # Expand mask token\n",
    "                projected_embeds # Keep original embedding if not masked\n",
    "            )\n",
    "\n",
    "        # --- 8. Prepend CLS Token ---\n",
    "        cls_token_expanded = self.cls_embedding.expand(batch_size, -1, -1)\n",
    "        full_embedding = torch.cat([cls_token_expanded, projected_embeds], dim=1) # Shape: (batch, seq_len+1, embedding_dim)\n",
    "\n",
    "        # --- 9. Create Final Padding Mask (for Transformer Attention) ---\n",
    "        # Includes position for CLS token (never padded)\n",
    "        final_padding_mask = torch.cat([\n",
    "             torch.zeros(batch_size, 1, dtype=torch.bool, device=device), # CLS padding (False)\n",
    "             padding_mask # Original sequence padding mask\n",
    "        ], dim=1) # Shape: (batch, seq_len+1)\n",
    "\n",
    "        # --- 10. Return necessary outputs ---\n",
    "        # output_mask is the boolean mask indicating which input tokens were masked (needed for loss)\n",
    "        # final_padding_mask indicates padding for the attention mechanism (includes CLS)\n",
    "        return full_embedding, final_padding_mask, output_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bda4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polarbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
