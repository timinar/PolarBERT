{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "769c021d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration from: /groups/pheno/inar/PolarBERT/configs/polarbert_new.yaml\n",
      "Validating configuration...\n",
      "Configuration validation passed (with potential warnings).\n"
     ]
    }
   ],
   "source": [
    "from polarbert.pretraining import get_dataloaders\n",
    "from polarbert.time_embedding import IceCubeTimeEmbedding\n",
    "from polarbert.config import PolarBertConfig\n",
    "\n",
    "config_file = \"/groups/pheno/inar/PolarBERT/configs/polarbert_new.yaml\"\n",
    "config = PolarBertConfig.from_yaml(config_file)\n",
    "\n",
    "config_dict = config.to_dict()\n",
    "config_dict[\"training\"][\"per_device_batch_size\"] = 1024\n",
    "train_loader, val_loader = get_dataloaders(config_dict, dataset_type='kaggle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a65bef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration from: /groups/pheno/inar/PolarBERT/configs/polarbert_new.yaml\n",
      "Validating configuration...\n",
      "Configuration validation passed (with potential warnings).\n",
      "{'data': {'max_per_device_batch_size': 4096,\n",
      "          'num_workers': 1,\n",
      "          'persistent_workers': True,\n",
      "          'pin_memory': False,\n",
      "          'train_dir': '/groups/pheno/inar/icecube_kaggle/memmaped_100M_127',\n",
      "          'train_events': 100000000,\n",
      "          'val_dir': '/groups/pheno/inar/icecube_kaggle/memmaped_eval_1M_127',\n",
      "          'val_events': 200000},\n",
      " 'model': {'dropout': 0.0,\n",
      "           'embedding': {'aux_embedding_dim': 4,\n",
      "                         'charge_bin_max': 0.9,\n",
      "                         'charge_bin_min': -0.6,\n",
      "                         'charge_embedding_dim': 16,\n",
      "                         'charge_vocab_size': 128,\n",
      "                         'dom_embedding_dim': 108,\n",
      "                         'dom_vocab_size': 5162,\n",
      "                         'embedding_dim': 256,\n",
      "                         'embedding_projection': False,\n",
      "                         'masking_charges': False,\n",
      "                         'masking_doms': True,\n",
      "                         'masking_prob': 0.25,\n",
      "                         'masking_times': False,\n",
      "                         'time_embedding_dim': 128,\n",
      "                         'time_vocab_size': 52000},\n",
      "           'embedding_dim': 256,\n",
      "           'ffd_type': 'SwiGLU',\n",
      "           'hidden_size': 1024,\n",
      "           'lambda_charge': 1.0,\n",
      "           'model_name': 'polarbert_time_embed',\n",
      "           'norm_eps': 1e-05,\n",
      "           'num_heads': 8,\n",
      "           'num_layers': 8,\n",
      "           'use_positional_embedding': False,\n",
      "           'use_rope': False},\n",
      " 'training': {'adam_beta1': 0.9,\n",
      "              'adam_beta2': 0.95,\n",
      "              'adam_eps': 1e-08,\n",
      "              'amsgrad': False,\n",
      "              'checkpoint': {'dirpath': 'checkpoints',\n",
      "                             'mode': 'min',\n",
      "                             'monitor': 'val/full_loss',\n",
      "                             'save_final': True,\n",
      "                             'save_last': True,\n",
      "                             'save_top_k': 1},\n",
      "              'div_factor': 25.0,\n",
      "              'final_div_factor': 10000.0,\n",
      "              'gpus': 1,\n",
      "              'gradient_clip_val': 1.0,\n",
      "              'logging': {'project': '2025-04-PolarBERT-time-embed'},\n",
      "              'logical_batch_size': 4096,\n",
      "              'lr_scheduler': 'onecycle',\n",
      "              'max_epochs': 20,\n",
      "              'max_lr': 0.0003,\n",
      "              'optimizer': 'AdamW',\n",
      "              'pct_start': 0.2,\n",
      "              'precision': '16-mixed',\n",
      "              'val_check_interval': 0.5,\n",
      "              'warm_up_steps': 1000,\n",
      "              'weight_decay': 0.1}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "config = PolarBertConfig.from_yaml(config_file)\n",
    "config_dict = config.to_dict()\n",
    "pprint(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e7542be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Concatenated embeddings directly match model embedding dim. No projection layer used.\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    break\n",
    "(x, l), (y, c) = batch\n",
    "\n",
    "embedding = IceCubeTimeEmbedding(config, masking=True)\n",
    "\n",
    "full_embedding, padding_mask, mask = embedding.forward((x,l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6de8661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import math\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# Assume PolarBertConfig and IceCubeTimeEmbedding are defined and imported\n",
    "# from polarbert.config import PolarBertConfig, ModelConfig # Import ModelConfig if separate\n",
    "# from polarbert.time_embedding import IceCubeTimeEmbedding\n",
    "\n",
    "# --- RMSNorm Implementation ---\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    \"\"\" Root Mean Square Layer Normalization \"\"\"\n",
    "    def __init__(self, dim: int, eps: float = 1e-6): # Default eps if not in config\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        # The gamma parameter\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        # Calculate sqrt(E[x^2] + eps)\n",
    "        # Original: return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        # Using separate steps for clarity:\n",
    "        rms = torch.sqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return x / rms\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normalize and scale by gamma\n",
    "        output = self._norm(x.float()).type_as(x) # Ensure float for norm, then cast back\n",
    "        return output * self.weight\n",
    "\n",
    "\n",
    "# --- Updated Transformer Components ---\n",
    "\n",
    "class PolarBertAttention(nn.Module):\n",
    "\n",
    "    # ... (using nn.MultiheadAttention -- should use flash attention)\n",
    "    \"\"\" Basic Multi-Head Attention using PyTorch's efficient implementation \"\"\"\n",
    "    def __init__(self, config: PolarBertConfig):\n",
    "        super().__init__()\n",
    "        model_cfg = config.model\n",
    "        self.embed_dim = model_cfg.embedding_dim\n",
    "        self.n_head = model_cfg.num_heads\n",
    "        self.dropout = model_cfg.dropout\n",
    "        assert self.embed_dim % self.n_head == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.mha = nn.MultiheadAttention(\n",
    "            embed_dim=self.embed_dim,\n",
    "            num_heads=self.n_head,\n",
    "            dropout=self.dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        attn_output, _ = self.mha(x, x, x, key_padding_mask=key_padding_mask, is_causal=False, need_weights=False)\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "class PolarBertFeedForward(nn.Module):\n",
    "    # ... (handling SwiGLU/MLP) ...\n",
    "    \"\"\" FeedForward network with choice of SwiGLU or MLP \"\"\"\n",
    "    def __init__(self, config: PolarBertConfig):\n",
    "        super().__init__()\n",
    "        model_cfg = config.model\n",
    "        emb_dim = model_cfg.embedding_dim\n",
    "        hidden_dim = model_cfg.hidden_size\n",
    "        dropout = model_cfg.dropout # TODO: remove completely?\n",
    "        self.ffd_type = model_cfg.ffd_type.lower()\n",
    "\n",
    "        if self.ffd_type == \"swiglu\":\n",
    "            self.w1 = nn.Linear(emb_dim, hidden_dim, bias=False)\n",
    "            self.w3 = nn.Linear(emb_dim, hidden_dim, bias=False)\n",
    "            self.w2 = nn.Linear(hidden_dim, emb_dim, bias=False)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        elif self.ffd_type == \"mlp\":\n",
    "             self.ffn = nn.Sequential(\n",
    "                nn.Linear(emb_dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, emb_dim),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported ffd_type: {model_cfg.ffd_type}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.ffd_type == \"swiglu\":\n",
    "            return self.dropout(self.w2(F.silu(self.w3(x)) * self.w1(x)))\n",
    "        else: # MLP\n",
    "            return self.ffn(x)\n",
    "\n",
    "\n",
    "class PolarBertBlock(nn.Module):\n",
    "    \"\"\" A single Transformer block using RMSNorm (Pre-Normalization) \"\"\"\n",
    "    def __init__(self, config: PolarBertConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        emb_dim = config.model.embedding_dim\n",
    "        norm_eps = config.model.norm_eps # Get eps from config\n",
    "        dropout = config.model.dropout # TODO: Remove?\n",
    "\n",
    "        # Use RMSNorm\n",
    "        self.ln1 = RMSNorm(emb_dim, eps=norm_eps)\n",
    "        self.attn = PolarBertAttention(config)\n",
    "        # Use RMSNorm\n",
    "        self.ln2 = RMSNorm(emb_dim, eps=norm_eps)\n",
    "        self.ffn = PolarBertFeedForward(config)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        # Pre-Norm: Norm -> Op -> Dropout -> Add Residual\n",
    "        # Attention part\n",
    "        residual = x\n",
    "        x_norm = self.ln1(x)\n",
    "        attn_output = self.attn(x_norm, key_padding_mask=key_padding_mask)\n",
    "        x = residual + self.dropout(attn_output)\n",
    "\n",
    "        # FeedForward part\n",
    "        residual = x\n",
    "        x_norm = self.ln2(x)\n",
    "        ffn_output = self.ffn(x_norm)\n",
    "        x = residual + self.dropout(ffn_output)\n",
    "        return x\n",
    "\n",
    "# --- Updated Main Model ---\n",
    "\n",
    "class PolarBertModel(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Main PolarBERT model using RMSNorm and custom components.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: PolarBertConfig):\n",
    "        super().__init__()\n",
    "        # Ensure config is validated if not done automatically by PolarBertConfig.__init__\n",
    "        # config._validate() # Assuming validation happens in PolarBertConfig __init__\n",
    "        self.config = config\n",
    "        # Important: Convert config object to dict for save_hyperparameters\n",
    "        # Ensure config.to_dict() method exists and works correctly\n",
    "        self.save_hyperparameters(config.to_dict())\n",
    "\n",
    "        # --- Modules ---\n",
    "        # Use masking=True for pre-training by default\n",
    "        self.embedding = IceCubeTimeEmbedding(config, masking=True)\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [PolarBertBlock(config) for _ in range(config.model.num_layers)]\n",
    "        )\n",
    "\n",
    "        # Use RMSNorm\n",
    "        self.final_norm = RMSNorm(config.model.embedding_dim, eps=config.model.norm_eps)\n",
    "\n",
    "        # --- Prediction Heads ---\n",
    "        # Predict original DOM ID (0 to N_DOMS-1)\n",
    "        num_dom_classes = config.model.embedding.dom_vocab_size - 2 # Exclude PAD, MASK\n",
    "        self.dom_head = nn.Linear(config.model.embedding_dim, num_dom_classes)\n",
    "\n",
    "        self.charge_head = nn.Linear(config.model.embedding_dim, 1)\n",
    "\n",
    "        # Store loss weight\n",
    "        self.lambda_charge = config.model.lambda_charge\n",
    "\n",
    "\n",
    "    def forward(self, batch: Tuple[Tuple[torch.Tensor, torch.Tensor], Optional[Tuple[torch.Tensor, torch.Tensor]]]) \\\n",
    "            -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor], torch.Tensor]:\n",
    "        \"\"\" Forward pass for pre-training. \"\"\"\n",
    "        (x, l), _ = batch\n",
    "        # batch_size, seq_len_orig variables removed as they were unused\n",
    "\n",
    "        # 1. Get Embeddings and Masks\n",
    "        hidden_states, final_padding_mask, output_mask = self.embedding((x, l))\n",
    "        attn_key_padding_mask = final_padding_mask # Use the mask covering CLS + Sequence\n",
    "\n",
    "        # TODO: Add RoPE or Positional Embeddings here if config enables them\n",
    "\n",
    "        # 2. Pass through Transformer Blocks\n",
    "        for block in self.transformer_blocks:\n",
    "            hidden_states = block(hidden_states, key_padding_mask=attn_key_padding_mask)\n",
    "\n",
    "        # 3. Final Normalization\n",
    "        hidden_states = self.final_norm(hidden_states)\n",
    "\n",
    "        # 4. Separate CLS and Sequence Embeddings\n",
    "        cls_embed = hidden_states[:, 0, :]         # (B, E)\n",
    "        sequence_embeds = hidden_states[:, 1:, :] # (B, L_orig, E)\n",
    "\n",
    "        # 5. Prediction Heads\n",
    "        charge_pred = self.charge_head(cls_embed)             # (B, 1)\n",
    "        dom_logits = self.dom_head(sequence_embeds)         # (B, L_orig, num_dom_classes)\n",
    "\n",
    "        # 6. Prepare Padding Mask for Loss Calculation (needs shape B, L_orig)\n",
    "        seq_padding_mask = final_padding_mask[:, 1:]\n",
    "\n",
    "        return dom_logits, charge_pred, output_mask, seq_padding_mask\n",
    "\n",
    "    def _calculate_loss(self, batch):\n",
    "        # (Keep implementation from previous response)\n",
    "        # ... (calculating dom_loss and charge_loss) ...\n",
    "        (x, l), y_data = batch\n",
    "        true_total_charge = y_data[1] if y_data is not None else None\n",
    "        true_dom_ids = x[:, :, 3].long()\n",
    "\n",
    "        dom_logits, charge_pred, output_mask, seq_padding_mask = self.forward(batch)\n",
    "\n",
    "        dom_loss = torch.tensor(0.0, device=dom_logits.device)\n",
    "        if output_mask is not None and output_mask.sum() > 0:\n",
    "             dom_targets = true_dom_ids - 1 # Map 1..N -> 0..N-1; Pad 0 -> -1\n",
    "             masked_logits = dom_logits[output_mask]\n",
    "             masked_targets = dom_targets[output_mask]\n",
    "             dom_loss = F.cross_entropy(masked_logits, masked_targets, ignore_index=-1)\n",
    "\n",
    "        charge_loss = torch.tensor(0.0, device=charge_pred.device)\n",
    "        if true_total_charge is not None:\n",
    "             true_log_charge = torch.log10(torch.clamp(true_total_charge.float(), min=1e-6))\n",
    "             charge_loss = F.mse_loss(charge_pred.squeeze(-1), true_log_charge)\n",
    "\n",
    "        combined_loss = dom_loss + self.lambda_charge * charge_loss\n",
    "        return combined_loss, dom_loss, charge_loss\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # (Keep implementation from previous response)\n",
    "        # ... (logging losses) ...\n",
    "        combined_loss, dom_loss, charge_loss = self._calculate_loss(batch)\n",
    "        self.log('train/loss', combined_loss, prog_bar=True, on_step=True, on_epoch=False, sync_dist=True)\n",
    "        self.log('train/dom_loss', dom_loss, on_step=True, on_epoch=False, sync_dist=True)\n",
    "        self.log('train/charge_loss', charge_loss, on_step=True, on_epoch=False, sync_dist=True)\n",
    "        return combined_loss\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # (Keep implementation from previous response)\n",
    "        # ... (logging losses) ...\n",
    "        combined_loss, dom_loss, charge_loss = self._calculate_loss(batch)\n",
    "        self.log('val/loss', combined_loss, prog_bar=True, sync_dist=True)\n",
    "        self.log('val/dom_loss', dom_loss, sync_dist=True)\n",
    "        self.log('val/charge_loss', charge_loss, sync_dist=True)\n",
    "        return combined_loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # ... (separating params for weight decay, creating optimizer and scheduler) ...\n",
    "        # Make sure to access config like self.config.training.weight_decay etc.\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        # ... (rest of weight decay logic as before) ...\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                 fpn = f'{mn}.{pn}' if mn else pn\n",
    "                 if not p.requires_grad: continue\n",
    "                 if pn.endswith('bias'): no_decay.add(fpn)\n",
    "                 elif pn.endswith('weight') and isinstance(m, nn.Linear): decay.add(fpn)\n",
    "                 # Updated check for RMSNorm/LayerNorm and Embedding weights\n",
    "                 elif pn.endswith('weight') and isinstance(m, (RMSNorm, nn.LayerNorm, nn.Embedding)): no_decay.add(fpn)\n",
    "                 elif 'norm' in pn.lower(): no_decay.add(fpn) # Catch potential norm weights by name\n",
    "                 elif pn in ['cls_embedding']: decay.add(fpn) # Decay CLS token\n",
    "\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
    "        # ... (validation assertions for decay sets) ...\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, f\"Parameters in both decay/no_decay: {inter_params}\"\n",
    "        unassigned_params = param_dict.keys() - union_params\n",
    "        if len(unassigned_params) > 0:\n",
    "            print(f\"WARNING: Parameters not assigned to weight decay groups: {unassigned_params}\")\n",
    "            no_decay.update(unassigned_params) # Assign leftovers to no_decay\n",
    "\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": self.config.training.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "\n",
    "        optimizer_name = self.config.training.optimizer.lower()\n",
    "        lr = self.config.training.max_lr\n",
    "\n",
    "        if optimizer_name == 'adamw':\n",
    "             optimizer = torch.optim.AdamW(\n",
    "                 optim_groups, lr=lr,\n",
    "                 betas=(self.config.training.adam_beta1, self.config.training.adam_beta2),\n",
    "                 eps=self.config.training.adam_eps,\n",
    "                 amsgrad=self.config.training.amsgrad\n",
    "            )\n",
    "        else: raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "\n",
    "        scheduler_name = self.config.training.lr_scheduler.lower()\n",
    "        if scheduler_name == 'onecycle':\n",
    "             if self.config.training.total_steps is None: raise ValueError(\"total_steps must be calculated.\")\n",
    "             scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "                 optimizer, max_lr=self.config.training.max_lr, total_steps=self.config.training.total_steps,\n",
    "                 pct_start=self.config.training.pct_start, div_factor=self.config.training.div_factor,\n",
    "                 final_div_factor=self.config.training.final_div_factor, anneal_strategy='cos'\n",
    "             )\n",
    "             return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n",
    "        elif scheduler_name in ['none', None]: return optimizer\n",
    "        else: raise ValueError(f\"Unsupported scheduler: {scheduler_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a19228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e721cf56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polarbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
