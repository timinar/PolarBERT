{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e61bb5ee",
   "metadata": {},
   "source": [
    "**TODO** Write proper tests using pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e0b7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import os\n",
    "import yaml # Make sure PyYAML is installed\n",
    "\n",
    "# --- Assume these are in your project structure ---\n",
    "# Make sure paths are correct or install your package\n",
    "try:\n",
    "    from polarbert.config import PolarBertConfig\n",
    "    # Assuming the new model is PolarBertModel and uses IceCubeTimeEmbedding\n",
    "    from polarbert.time_embed_polarbert import PolarBertModel\n",
    "    from polarbert.time_embedding import IceCubeTimeEmbedding # Or the actual name you used\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing necessary modules: {e}\")\n",
    "    print(\"Please ensure your classes PolarBertConfig, PolarBertModel, IceCubeTimeEmbedding are accessible.\")\n",
    "    # You might need to adjust sys.path or install your package (`pip install -e .`)\n",
    "    # import sys\n",
    "    # sys.path.append('/path/to/your/src') # Example if not installed\n",
    "    # from config import PolarBertConfig\n",
    "    # from model import PolarBertModel\n",
    "    # from time_embedding import IceCubeTimeEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d605a448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sanity Check Setup ---\n",
      "Loading configuration from: /groups/pheno/inar/PolarBERT/configs/polarbert_new.yaml\n",
      "Validating configuration...\n",
      "Configuration validation passed (with potential warnings).\n",
      "Successfully loaded config from /groups/pheno/inar/PolarBERT/configs/polarbert_new.yaml\n",
      "Using CUDA device 0: NVIDIA H100 NVL\n",
      "Created dummy batch: x shape torch.Size([4, 127, 4]), l shape torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Test Setup ---\n",
    "print(\"--- Sanity Check Setup ---\")\n",
    "# 1. Load Config\n",
    "# Make sure this path points to your YAML file\n",
    "config_file = \"/groups/pheno/inar/PolarBERT/configs/polarbert_new.yaml\"\n",
    "try:\n",
    "    config = PolarBertConfig.from_yaml(config_file)\n",
    "    print(f\"Successfully loaded config from {config_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"FATAL: Could not load config file: {e}\")\n",
    "    # Stop execution or handle error\n",
    "    raise e\n",
    "\n",
    "# 2. Define Device\n",
    "if torch.cuda.is_available():\n",
    "    # Explicitly use cuda:0\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"Using CUDA device 0: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available, using CPU.\")\n",
    "\n",
    "\n",
    "# 3. Create a Dummy Batch\n",
    "B, L_orig, Ff = 4, 127, 4 # Batch size, Original Seq Len, Num Features\n",
    "dummy_x = torch.rand(B, L_orig, Ff, dtype=torch.float32)\n",
    "# Simulate some padding (last 10 elements for batch 0, last 20 for batch 1 etc.)\n",
    "dummy_l = torch.tensor([L_orig - 10, L_orig - 20, L_orig - 5, L_orig], dtype=torch.long)\n",
    "for i in range(B):\n",
    "    if dummy_l[i] < L_orig:\n",
    "        dummy_x[i, dummy_l[i]:, :] = 0.0 # Set padding features to 0\n",
    "        dummy_x[i, dummy_l[i]:, 3] = 0.0 # Ensure DOM ID is 0 for padding\n",
    "\n",
    "# Simulate some plausible feature values (replace rand if needed)\n",
    "# Feature 3 (DOM ID): Should be > 0 for non-padded\n",
    "non_pad_mask_init = (dummy_x[:, :, 3] != 0)\n",
    "num_non_pad = non_pad_mask_init.sum()\n",
    "dummy_x[:,:,3][non_pad_mask_init] = torch.randint(1, 5161, size=(num_non_pad,)).float() + 1.0 # DOM ID + 1\n",
    "\n",
    "# Feature 2 (Aux): -0.5 or 0.5\n",
    "aux_vals = torch.randint(0, 2, size=(num_non_pad,)).float() - 0.5\n",
    "dummy_x[:,:,2][non_pad_mask_init] = aux_vals\n",
    "\n",
    "# Add dummy labels (y, c) - required for _calculate_loss / training_step\n",
    "# Shape depends on what your _calculate_loss expects from y_data tuple\n",
    "# Example: y_data = (y_angles_ignored, true_total_charge)\n",
    "dummy_y_angles = torch.rand(B, 2) # Placeholder, not used in pretrain loss\n",
    "dummy_charge_target = torch.rand(B) * 1000 + 1 # Example total charges\n",
    "dummy_y_data = (dummy_y_angles, dummy_charge_target)\n",
    "\n",
    "dummy_batch = ((dummy_x, dummy_l), dummy_y_data)\n",
    "print(f\"Created dummy batch: x shape {dummy_x.shape}, l shape {dummy_l.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf82c82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test 1: Initialization ---\n",
      "INFO: Concatenated embeddings directly match model embedding dim. No projection layer used.\n",
      "PASS: Model initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Test 1: Initialization ---\n",
    "print(\"\\n--- Test 1: Initialization ---\")\n",
    "try:\n",
    "    # Ensure mask_prob exists in config if masking=True\n",
    "    # We'll force masking=True for thorough tests\n",
    "    # config.model.embedding.masking_prob = 0.15 # Make sure it's set\n",
    "    model = PolarBertModel(config) # Assumes masking is handled by config\n",
    "    print(\"PASS: Model initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"FAIL: Model initialization failed: {e}\")\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "201d89b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test 2: Device Placement ---\n",
      "Target device: cuda:0 (type: <class 'torch.device'>)\n",
      "Model parameter device: cuda:0 (type: <class 'torch.device'>)\n",
      "PASS: Model and batch moved to cuda:0.\n"
     ]
    }
   ],
   "source": [
    "# --- Test 2: Device Placement ---\n",
    "print(\"\\n--- Test 2: Device Placement ---\")\n",
    "try:\n",
    "    model.to(device)\n",
    "    # Move all parts of the batch to the device\n",
    "    dummy_x_dev = dummy_x.to(device)\n",
    "    dummy_l_dev = dummy_l.to(device)\n",
    "    dummy_y_angles_dev = dummy_y_angles.to(device)\n",
    "    dummy_charge_target_dev = dummy_charge_target.to(device)\n",
    "    dummy_batch_dev = ((dummy_x_dev, dummy_l_dev), (dummy_y_angles_dev, dummy_charge_target_dev))\n",
    "\n",
    "    # --- Debugging Print ---\n",
    "    model_device = next(model.parameters()).device\n",
    "    print(f\"Target device: {device} (type: {type(device)})\")\n",
    "    print(f\"Model parameter device: {model_device} (type: {type(model_device)})\")\n",
    "    # --- End Debugging Print ---\n",
    "\n",
    "    assert model_device == device # Check model device\n",
    "    assert dummy_batch_dev[0][0].device == device # Check input x device\n",
    "    assert dummy_batch_dev[0][1].device == device # Check input l device\n",
    "    assert dummy_batch_dev[1][1].device == device # Check target c device\n",
    "    print(f\"PASS: Model and batch moved to {device}.\")\n",
    "except Exception as e:\n",
    "    print(f\"FAIL: Moving model/batch to device failed: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61f50243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test 3: Forward Pass & Shapes (Eval Mode) ---\n",
      "Output dtypes: dom=torch.float32, charge=torch.float32\n",
      "PASS: Forward pass successful, shapes and device are correct.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Test 3: Forward Pass & Shapes (Eval Mode) ---\n",
    "print(\"\\n--- Test 3: Forward Pass & Shapes (Eval Mode) ---\")\n",
    "model.eval() # Set to evaluation mode (disables dropout)\n",
    "try:\n",
    "    with torch.no_grad(): # No need to track gradients here\n",
    "        dom_logits, charge_pred, output_mask, seq_padding_mask = model.forward(dummy_batch_dev)\n",
    "\n",
    "    # Check shapes\n",
    "    E = config.model.embedding_dim\n",
    "    num_dom_classes = config.model.embedding.dom_vocab_size - 2\n",
    "    expected_dom_shape = (B, L_orig, num_dom_classes)\n",
    "    expected_charge_shape = (B, 1)\n",
    "    expected_mask_shape = (B, L_orig) # Output mask has original length\n",
    "    expected_pad_mask_shape = (B, L_orig) # Seq padding mask has original length\n",
    "\n",
    "    assert dom_logits.shape == expected_dom_shape, f\"DOM Logits shape mismatch: Expected {expected_dom_shape}, Got {dom_logits.shape}\"\n",
    "    assert charge_pred.shape == expected_charge_shape, f\"Charge Pred shape mismatch: Expected {expected_charge_shape}, Got {charge_pred.shape}\"\n",
    "    # Output mask might be None if masking is off in config/init, handle this\n",
    "    if hasattr(model.embedding, 'masking') and model.embedding.masking:\n",
    "         assert output_mask is not None, \"Output mask should not be None when masking is enabled\"\n",
    "         assert output_mask.shape == expected_mask_shape, f\"Output Mask shape mismatch: Expected {expected_mask_shape}, Got {output_mask.shape}\"\n",
    "    else:\n",
    "         assert output_mask is None, \"Output mask should be None when masking is disabled\"\n",
    "    assert seq_padding_mask.shape == expected_pad_mask_shape, f\"Seq Padding Mask shape mismatch: Expected {expected_pad_mask_shape}, Got {seq_padding_mask.shape}\"\n",
    "\n",
    "    # Check device and dtype\n",
    "    assert dom_logits.device == device, \"Output device mismatch\"\n",
    "    # Dtype might depend on precision setting ('16-mixed' might output float16)\n",
    "    print(f\"Output dtypes: dom={dom_logits.dtype}, charge={charge_pred.dtype}\")\n",
    "\n",
    "    print(\"PASS: Forward pass successful, shapes and device are correct.\")\n",
    "    # Store outputs for next test\n",
    "    outputs1 = (dom_logits, charge_pred, output_mask, seq_padding_mask)\n",
    "except Exception as e:\n",
    "    print(f\"FAIL: Forward pass failed: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b1bd611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test 4: Batch Consistency (Eval Mode) ---\n",
      "Running first forward pass in eval mode (model.training=False)...\n",
      "Running second forward pass in eval mode (model.training=False)...\n",
      "Output masks are identical: False\n",
      "FAIL: Consistency check failed: Output masks differ between runs!\n"
     ]
    }
   ],
   "source": [
    "# --- Test 4: Batch Consistency (Eval Mode) ---\n",
    "print(\"\\n--- Test 4: Batch Consistency (Eval Mode) ---\")\n",
    "try:\n",
    "    print(f\"Running first forward pass in eval mode (model.training={model.training})...\")\n",
    "    assert not model.training # Double check eval mode is set\n",
    "    with torch.no_grad():\n",
    "        outputs1 = model.forward(dummy_batch_dev)\n",
    "\n",
    "    print(f\"Running second forward pass in eval mode (model.training={model.training})...\")\n",
    "    assert not model.training # Double check eval mode is set\n",
    "    with torch.no_grad():\n",
    "        outputs2 = model.forward(dummy_batch_dev)\n",
    "\n",
    "    # --- Detailed Comparison ---\n",
    "    dom_logits1, charge_pred1, output_mask1, seq_padding_mask1 = outputs1\n",
    "    dom_logits2, charge_pred2, output_mask2, seq_padding_mask2 = outputs2\n",
    "\n",
    "    # 1. Compare Output Masks (Boolean)\n",
    "    mask_identical = False\n",
    "    if output_mask1 is not None and output_mask2 is not None:\n",
    "        mask_identical = torch.equal(output_mask1, output_mask2)\n",
    "        print(f\"Output masks are identical: {mask_identical}\")\n",
    "        assert mask_identical, \"Output masks differ between runs!\"\n",
    "    elif output_mask1 is None and output_mask2 is None:\n",
    "        mask_identical = True # Both None is consistent\n",
    "        print(\"Output masks are identical (both None)\")\n",
    "    else:\n",
    "        print(f\"FAIL: Output mask presence mismatch ({type(output_mask1)} vs {type(output_mask2)})\")\n",
    "        assert False, \"Output mask presence mismatch\"\n",
    "\n",
    "    # 2. Compare Padding Masks (Boolean)\n",
    "    pad_mask_identical = torch.equal(seq_padding_mask1, seq_padding_mask2)\n",
    "    print(f\"Seq padding masks are identical: {pad_mask_identical}\")\n",
    "    assert pad_mask_identical, \"Seq padding masks differ between runs!\"\n",
    "\n",
    "    # 3. Compare Charge Predictions (Float)\n",
    "    charge_diff = torch.abs(charge_pred1 - charge_pred2).max().item()\n",
    "    charge_close = torch.allclose(charge_pred1, charge_pred2, atol=1e-6)\n",
    "    print(f\"Charge predictions are allclose (atol=1e-6): {charge_close} (Max Abs Diff: {charge_diff:.2e})\")\n",
    "    assert charge_close, \"Charge predictions differ between runs\"\n",
    "\n",
    "    # 4. Compare DOM Logits (Float) - Fails Here\n",
    "    dom_diff = torch.abs(dom_logits1 - dom_logits2).max().item()\n",
    "    dom_close_strict = torch.allclose(dom_logits1, dom_logits2, atol=1e-6)\n",
    "    dom_close_loose = torch.allclose(dom_logits1, dom_logits2, atol=1e-1) # The one that failed\n",
    "    print(f\"DOM logits are allclose (atol=1e-6): {dom_close_strict} (Max Abs Diff: {dom_diff:.2e})\")\n",
    "    print(f\"DOM logits are allclose (atol=1e-1): {dom_close_loose}\")\n",
    "\n",
    "    # Raise the original error if the loose check failed\n",
    "    assert dom_close_loose, \"DOM logits differ between runs (even with atol=1e-1)\"\n",
    "\n",
    "    print(\"PASS: Model output is consistent for the same input in eval mode.\") # Will only reach here if all asserts pass\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"FAIL: Consistency check failed: {e}\")\n",
    "    # Don't re-raise immediately, let the prints show results\n",
    "    # raise e # Optionally re-raise after printing debug info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48c5ac0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test 5: Loss Calculation (Train Mode) ---\n",
      "PASS: Loss calculated successfully: Combined=14.1082, DOM=8.6455, Charge=5.4627\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Test 5: Loss Calculation (Train Mode) ---\n",
    "print(\"\\n--- Test 5: Loss Calculation (Train Mode) ---\")\n",
    "model.train() # Set to training mode\n",
    "try:\n",
    "    # Can call _calculate_loss directly or training_step\n",
    "    # training_step requires a batch_idx\n",
    "    combined_loss, dom_loss, charge_loss = model._calculate_loss(dummy_batch_dev)\n",
    "    # loss_from_step = model.training_step(dummy_batch_dev, batch_idx=0)\n",
    "\n",
    "    # Check if loss is a scalar tensor\n",
    "    assert isinstance(combined_loss, torch.Tensor), \"Loss is not a Tensor\"\n",
    "    assert combined_loss.ndim == 0, f\"Loss should be scalar, but has shape {combined_loss.shape}\"\n",
    "    assert combined_loss.requires_grad, \"Loss does not require gradients\"\n",
    "    # Check if loss is finite\n",
    "    assert torch.isfinite(combined_loss), \"Loss is not finite!\"\n",
    "\n",
    "    print(f\"PASS: Loss calculated successfully: Combined={combined_loss.item():.4f}, DOM={dom_loss.item():.4f}, Charge={charge_loss.item():.4f}\")\n",
    "    loss_for_backward = combined_loss # Store for next test\n",
    "except Exception as e:\n",
    "    print(f\"FAIL: Loss calculation failed: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4baf463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test 6: Basic Gradient Flow (Train Mode) ---\n",
      "Gradient found for embedding.cls_embedding: mean abs = 0.14659950137138367\n",
      "PASS: Backward pass executed and gradients seem to be present.\n",
      "\n",
      "--- Sanity Checks Complete ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Test 6: Basic Gradient Flow (Train Mode) ---\n",
    "print(\"\\n--- Test 6: Basic Gradient Flow (Train Mode) ---\")\n",
    "model.train()\n",
    "# Use a simple optimizer just for the test\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "try:\n",
    "    optimizer.zero_grad()\n",
    "    loss_for_backward.backward()\n",
    "    optimizer.step() # Optional: check if step works\n",
    "\n",
    "    # Check if a key parameter has a gradient\n",
    "    # Example: first weight of the CLS embedding or a weight in the first block's attention\n",
    "    checked_grad = False\n",
    "    if model.embedding.cls_embedding.grad is not None:\n",
    "         print(f\"Gradient found for embedding.cls_embedding: mean abs = {model.embedding.cls_embedding.grad.abs().mean().item()}\")\n",
    "         checked_grad = True\n",
    "    # Check grad of a weight in the first transformer block's attention input projection\n",
    "    elif hasattr(model, 'transformer_blocks') and len(model.transformer_blocks) > 0 and hasattr(model.transformer_blocks[0], 'attn'):\n",
    "         attn_layer = model.transformer_blocks[0].attn.mha # Get the underlying MHA\n",
    "         if attn_layer.in_proj_weight.grad is not None:\n",
    "              print(f\"Gradient found for transformer_blocks[0].attn.mha.in_proj_weight: mean abs = {attn_layer.in_proj_weight.grad.abs().mean().item()}\")\n",
    "              checked_grad = True\n",
    "\n",
    "    if not checked_grad:\n",
    "         print(\"Warning: Could not easily check gradient for standard layers. Check manually.\")\n",
    "         # You might need to inspect other parameters like linear layer weights.\n",
    "         # Check if *any* parameter has a non-None gradient\n",
    "         found_any_grad = any(p.grad is not None for p in model.parameters() if p.requires_grad)\n",
    "         assert found_any_grad, \"No gradients found after backward pass!\"\n",
    "         print(\"OK: At least one parameter received gradients.\")\n",
    "\n",
    "\n",
    "    print(\"PASS: Backward pass executed and gradients seem to be present.\")\n",
    "    optimizer.zero_grad() # Clean up gradients\n",
    "except Exception as e:\n",
    "    print(f\"FAIL: Backward pass or gradient check failed: {e}\")\n",
    "    raise e\n",
    "\n",
    "print(\"\\n--- Sanity Checks Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a154de6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polarbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
