{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb2124ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f065cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Assuming your project structure allows these imports ---\n",
    "try:\n",
    "    # Import necessary classes from your project\n",
    "    from polarbert.config import PolarBertConfig\n",
    "    from polarbert.time_embed_polarbert import PolarBertModel # Pre-training model class\n",
    "    from polarbert.te_finetuning import PolarBertFinetuner # Fine-tuning model class\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing project modules: {e}\")\n",
    "    print(\"Please ensure your notebook's environment can access the polarbert package.\")\n",
    "    # You might need to add the source directory to sys.path\n",
    "    # import sys\n",
    "    # sys.path.append('/path/to/your/PolarBERT/src') # Adjust path if needed\n",
    "    # from polarbert.config import PolarBertConfig\n",
    "    # ... etc.\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "693ba525",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Define Checkpoint Paths ---\n",
    "pretrained_ckpt_path = \"/groups/pheno/inar/PolarBERT/checkpoints/te_polarbert_100M_2ep_250414-011046/last.ckpt\"\n",
    "finetuned_ckpt_path = \"/groups/pheno/inar/PolarBERT/checkpoints/finetune_direction_kaggle_250428-114235/last.ckpt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d160846f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Pre-trained Model ---\n",
      "Checkpoint: /groups/pheno/inar/PolarBERT/checkpoints/te_polarbert_100M_2ep_250414-011046/last.ckpt\n",
      "Loading config associated with checkpoint last.ckpt\n",
      "Loading configuration from: /groups/pheno/inar/PolarBERT/checkpoints/te_polarbert_100M_2ep_250414-011046/config.yaml\n",
      "Validating configuration...\n",
      "Configuration validation passed (with potential warnings).\n",
      "Pre-trained config loaded.\n",
      "INFO: Concatenated embeddings directly match model embedding dim. No projection layer used.\n",
      "Pre-trained model (PolarBertModel) instantiated.\n",
      "Pre-trained state dict loaded into model instance.\n",
      "\n",
      "Pre-trained Model Named Modules:\n",
      "                                 | PolarBertModel\n",
      "  embedding                      | IceCubeTimeEmbedding\n",
      "  transformer_blocks             | ModuleList\n",
      "  final_norm                     | RMSNorm\n",
      "  dom_head                       | Linear | Params: 1,326,120\n",
      "  charge_head                    | Linear | Params: 257\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Helper Function to Load State Dict ---\n",
    "# (Handles potential 'state_dict' nesting in checkpoint file)\n",
    "def load_state_dict_from_checkpoint(ckpt_path):\n",
    "    if not os.path.isfile(ckpt_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {ckpt_path}\")\n",
    "    try:\n",
    "        # Use weights_only=True for security\n",
    "        checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=True)\n",
    "        # Check if state_dict is nested (common in PL checkpoints)\n",
    "        if 'state_dict' in checkpoint:\n",
    "            return checkpoint['state_dict']\n",
    "        else:\n",
    "            # Assume the checkpoint directly contains the state_dict\n",
    "            return checkpoint\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading state dictionary from {ckpt_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "# --- 1. Load and Inspect Pre-trained Model ---\n",
    "print(f\"--- Loading Pre-trained Model ---\")\n",
    "print(f\"Checkpoint: {pretrained_ckpt_path}\")\n",
    "\n",
    "try:\n",
    "    # Load config associated with the pre-trained checkpoint\n",
    "    # Assumes config.yaml is in the same directory\n",
    "    pretrained_config = PolarBertConfig.from_checkpoint(pretrained_ckpt_path)\n",
    "    print(\"Pre-trained config loaded.\")\n",
    "\n",
    "    # Instantiate the pre-training model architecture\n",
    "    pretrained_model = PolarBertModel(pretrained_config)\n",
    "    print(\"Pre-trained model (PolarBertModel) instantiated.\")\n",
    "\n",
    "    # Load the state dict\n",
    "    pretrained_state_dict = load_state_dict_from_checkpoint(pretrained_ckpt_path)\n",
    "\n",
    "    # Load weights into the model instance\n",
    "    # Use strict=False initially to see missing/unexpected keys easily\n",
    "    missing_keys, unexpected_keys = pretrained_model.load_state_dict(\n",
    "        pretrained_state_dict, strict=False\n",
    "    )\n",
    "    print(\"Pre-trained state dict loaded into model instance.\")\n",
    "    if missing_keys:\n",
    "        print(f\"  Missing keys: {missing_keys}\")\n",
    "    if unexpected_keys:\n",
    "        print(f\"  Unexpected keys: {unexpected_keys}\") # Should ideally be empty\n",
    "\n",
    "    print(\"\\nPre-trained Model Named Modules:\")\n",
    "    # List top-level modules and prediction heads specifically\n",
    "    found_modules = []\n",
    "    for name, module in pretrained_model.named_modules():\n",
    "        # Print top-level modules or specific heads of interest\n",
    "        if '.' not in name or name in ['dom_head', 'charge_head', 'embedding', 'final_norm']:\n",
    "             module_info = f\"  {name:<30} | {module.__class__.__name__}\"\n",
    "             # Add parameter count for Linear layers\n",
    "             if isinstance(module, torch.nn.Linear):\n",
    "                 num_params = sum(p.numel() for p in module.parameters())\n",
    "                 module_info += f\" | Params: {num_params:,}\"\n",
    "             print(module_info)\n",
    "             found_modules.append(name)\n",
    "    if not found_modules:\n",
    "        print(\"  (Could not list modules - check model definition)\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"!!! Error loading pre-trained model: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0149f280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Fine-tuned Model ---\n",
      "Checkpoint: /groups/pheno/inar/PolarBERT/checkpoints/finetune_direction_kaggle_250428-114235/last.ckpt\n",
      "Loading config associated with checkpoint last.ckpt\n",
      "Loading configuration from: /groups/pheno/inar/PolarBERT/checkpoints/finetune_direction_kaggle_250428-114235/config.yaml\n",
      "Validating configuration...\n",
      "Configuration validation passed (with potential warnings).\n",
      "Fine-tuned config loaded.\n",
      "INFO: Concatenated embeddings directly match model embedding dim. No projection layer used.\n",
      "No pretrained checkpoint provided or 'new' specified. Training backbone from scratch.\n",
      "Initialized head for task: direction with hidden size: 1024\n",
      "Fine-tuned model (PolarBertFinetuner) instantiated.\n",
      "Fine-tuned state dict loaded into model instance.\n",
      "\n",
      "Fine-tuned Model Named Modules:\n",
      "                                 | PolarBertFinetuner\n",
      "  backbone                       | PolarBertModel\n",
      "  backbone.embedding             | IceCubeTimeEmbedding\n",
      "  backbone.transformer_blocks    | ModuleList\n",
      "  backbone.final_norm            | RMSNorm\n",
      "  backbone.dom_head              | Linear | Params: 1,326,120\n",
      "  backbone.charge_head           | Linear | Params: 257\n",
      "  prediction_head                | Sequential\n",
      "  prediction_head.0              | Linear | Params: 263,168\n",
      "  prediction_head.1              | ReLU\n",
      "  prediction_head.2              | Linear | Params: 3,075\n",
      "\n",
      "--- Inspection Complete ---\n",
      "Compare the named modules above.\n",
      "Key things to look for:\n",
      " - Pre-trained model: Should have `embedding`, `transformer_blocks`, `final_norm`, `dom_head`, `charge_head`.\n",
      " - Fine-tuned model: Should have `backbone` (containing embedding, blocks, norm) and `prediction_head` (likely an nn.Sequential). It should *not* have top-level `dom_head` or `charge_head`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 2. Load and Inspect Fine-tuned Model ---\n",
    "print(f\"\\n--- Loading Fine-tuned Model ---\")\n",
    "print(f\"Checkpoint: {finetuned_ckpt_path}\")\n",
    "\n",
    "try:\n",
    "    # Load config associated with the fine-tuned checkpoint\n",
    "    # Assumes config.yaml is in the same directory\n",
    "    finetuned_config = PolarBertConfig.from_checkpoint(finetuned_ckpt_path)\n",
    "    print(\"Fine-tuned config loaded.\")\n",
    "\n",
    "    # Instantiate the fine-tuning model architecture\n",
    "    # Pass None for pretrained_checkpoint_path to prevent auto-loading backbone here\n",
    "    # We will load the *entire* fine-tuned state dict afterwards\n",
    "    finetuned_model = PolarBertFinetuner(finetuned_config, pretrained_checkpoint_path=None)\n",
    "    print(\"Fine-tuned model (PolarBertFinetuner) instantiated.\")\n",
    "\n",
    "    # Load the state dict from the fine-tuned checkpoint\n",
    "    finetuned_state_dict = load_state_dict_from_checkpoint(finetuned_ckpt_path)\n",
    "\n",
    "    # Load weights into the model instance\n",
    "    # Use strict=False for inspection\n",
    "    missing_keys_ft, unexpected_keys_ft = finetuned_model.load_state_dict(\n",
    "        finetuned_state_dict, strict=False\n",
    "    )\n",
    "    print(\"Fine-tuned state dict loaded into model instance.\")\n",
    "    if missing_keys_ft:\n",
    "        print(f\"  Missing keys: {missing_keys_ft}\") # Should ideally be empty\n",
    "    if unexpected_keys_ft:\n",
    "        print(f\"  Unexpected keys: {unexpected_keys_ft}\") # Should ideally be empty\n",
    "\n",
    "    print(\"\\nFine-tuned Model Named Modules:\")\n",
    "    # List top-level modules and heads specifically\n",
    "    found_modules_ft = []\n",
    "    for name, module in finetuned_model.named_modules():\n",
    "         # Print top-level modules or specific parts like 'backbone' or 'prediction_head'\n",
    "         if '.' not in name or name in ['backbone', 'prediction_head'] or 'backbone.' in name and name.count('.')==1 or 'prediction_head.' in name :\n",
    "              module_info = f\"  {name:<30} | {module.__class__.__name__}\"\n",
    "              if isinstance(module, torch.nn.Linear):\n",
    "                   num_params = sum(p.numel() for p in module.parameters())\n",
    "                   module_info += f\" | Params: {num_params:,}\"\n",
    "              # Avoid printing every single layer inside backbone unless desired\n",
    "              if name.startswith('backbone') and name.count('.') > 1:\n",
    "                   # Example: skip printing individual transformer blocks etc.\n",
    "                   # Add more specific checks if needed\n",
    "                   if 'transformer_blocks.' in name or 'embedding.' in name:\n",
    "                       continue\n",
    "              print(module_info)\n",
    "              found_modules_ft.append(name)\n",
    "\n",
    "    if not found_modules_ft:\n",
    "        print(\"  (Could not list modules - check model definition)\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"!!! Error loading fine-tuned model: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n--- Inspection Complete ---\")\n",
    "print(\"Compare the named modules above.\")\n",
    "print(\"Key things to look for:\")\n",
    "print(\" - Pre-trained model: Should have `embedding`, `transformer_blocks`, `final_norm`, `dom_head`, `charge_head`.\")\n",
    "print(\" - Fine-tuned model: Should have `backbone` (containing embedding, blocks, norm) and `prediction_head` (likely an nn.Sequential). It should *not* have top-level `dom_head` or `charge_head`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88c502d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Setting up validation dataloader...\n",
      "Directory: /groups/pheno/inar/icecube_kaggle/memmaped_eval_1M_127\n",
      "Event limit: 100000\n",
      "Batch size: 1024\n",
      "Dataset sliced to 100000 events.\n",
      "Validation dataloader created.\n",
      "Estimated number of validation batches: 96\n",
      "\n",
      "Calculating DOM loss for: Pre-trained Model (DOM Head)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59561d6abd94445a8b8751207564e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval DOM Loss (Pre-trained Model (DOM Head)):   0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/hpc/pheno/inar/mambaforge/envs/polarbert/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:70: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1728945388353/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return torch.as_tensor(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete for Pre-trained Model (DOM Head). Batches with valid DOM loss: 96/96\n",
      "\n",
      "Calculating DOM loss for: Fine-tuned Model (Backbone DOM Head)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4feab6dfdf9b481096367ddb60039f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval DOM Loss (Fine-tuned Model (Backbone DOM Head)):   0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/hpc/pheno/inar/mambaforge/envs/polarbert/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:70: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1728945388353/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return torch.as_tensor(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete for Fine-tuned Model (Backbone DOM Head). Batches with valid DOM loss: 97/97\n",
      "\n",
      "--- DOM Loss Comparison ---\n",
      "Validation Dataset: /groups/pheno/inar/icecube_kaggle/memmaped_eval_1M_127 (100000 events)\n",
      "Pre-trained Model DOM Loss : 2.084680\n",
      "Fine-tuned Model DOM Loss  : 9.516705 (Using DOM Head within its backbone)\n",
      "---------------------------\n",
      "\n",
      "Note: DOM loss increased after fine-tuning, which might be expected if the fine-tuning task diverges significantly.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8af58937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Comparing DOM Head Parameters ---\n",
      "DOM Head Weights are the same: False\n",
      "DOM Head Biases are the same:  False\n",
      "\n",
      "Conclusion: The DOM head parameters are DIFFERENT between the two models.\n",
      "This difference in head weights likely contributes significantly to the difference in DOM loss.\n",
      "  Mean absolute difference in weights: 6.6025e-02\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# --- Ensure models are loaded from previous cells ---\n",
    "if 'pretrained_model' not in locals() or 'finetuned_model' not in locals():\n",
    "    print(\"Models not found in local scope. Please re-run the model loading cells first.\")\n",
    "else:\n",
    "    print(\"--- Comparing DOM Head Parameters ---\")\n",
    "\n",
    "    # Access the dom_head layers\n",
    "    # Note: For the fine-tuned model, the dom_head is part of its 'backbone' attribute\n",
    "    dom_head_pretrained = pretrained_model.dom_head\n",
    "    dom_head_finetuned = finetuned_model.backbone.dom_head # Access within the backbone\n",
    "\n",
    "    # Check if layers exist (should do based on inspection)\n",
    "    if not isinstance(dom_head_pretrained, torch.nn.Linear):\n",
    "         print(\"ERROR: Pre-trained model does not have an nn.Linear dom_head.\")\n",
    "    elif not isinstance(dom_head_finetuned, torch.nn.Linear):\n",
    "         print(\"ERROR: Fine-tuned model's backbone does not have an nn.Linear dom_head.\")\n",
    "    else:\n",
    "        # Compare Weights\n",
    "        weights_equal = torch.equal(dom_head_pretrained.weight.data, dom_head_finetuned.weight.data)\n",
    "        print(f\"DOM Head Weights are the same: {weights_equal}\")\n",
    "\n",
    "        # Compare Biases (check if bias exists for both first)\n",
    "        if dom_head_pretrained.bias is not None and dom_head_finetuned.bias is not None:\n",
    "            biases_equal = torch.equal(dom_head_pretrained.bias.data, dom_head_finetuned.bias.data)\n",
    "            print(f\"DOM Head Biases are the same:  {biases_equal}\")\n",
    "        elif dom_head_pretrained.bias is None and dom_head_finetuned.bias is None:\n",
    "            print(f\"DOM Head Biases are the same:  True (both are None)\")\n",
    "        else:\n",
    "            print(f\"DOM Head Biases are the same:  False (one has bias, the other doesn't)\")\n",
    "\n",
    "        # Overall conclusion\n",
    "        if weights_equal and \\\n",
    "           (dom_head_pretrained.bias is None and dom_head_finetuned.bias is None or\n",
    "            (dom_head_pretrained.bias is not None and dom_head_finetuned.bias is not None and biases_equal)):\n",
    "            print(\"\\nConclusion: The DOM head parameters are identical in both models.\")\n",
    "            print(\"The difference in DOM loss is likely due to differences in the backbone representations feeding into the head.\")\n",
    "        else:\n",
    "            print(\"\\nConclusion: The DOM head parameters are DIFFERENT between the two models.\")\n",
    "            print(\"This difference in head weights likely contributes significantly to the difference in DOM loss.\")\n",
    "            if not weights_equal:\n",
    "                 # Optional: Check the magnitude of difference\n",
    "                 weight_diff = torch.abs(dom_head_pretrained.weight.data - dom_head_finetuned.weight.data).mean()\n",
    "                 print(f\"  Mean absolute difference in weights: {weight_diff.item():.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4469a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Replacing Pre-training Heads in Fine-tuned Model's Backbone ---\n",
      "Original heads BEFORE replacement:\n",
      "  Pretrained dom_head ID: 139796762497808\n",
      "  Finetuned backbone dom_head ID: 139796762734800\n",
      "  DOM Head weights originally equal: False\n",
      "\n",
      "Assigning pre-trained heads to fine-tuned model's backbone...\n",
      "  - Assigned pretrained dom_head.\n",
      "  - Assigned pretrained charge_head.\n",
      "Assignment complete.\n",
      "\n",
      "Verifying heads AFTER replacement:\n",
      "  Pretrained dom_head ID: 139796762497808\n",
      "  Finetuned backbone dom_head ID (should match above): 139796762497808\n",
      "  DOM Head weights now equal: True\n",
      "  DOM Head biases now equal:  True\n",
      "  Verification successful: Fine-tuned backbone now uses identical pre-trained DOM head object.\n",
      "\n",
      "(Optional) Re-evaluating DOM loss on modified fine-tuned model...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn # Ensure nn is imported\n",
    "\n",
    "# --- Ensure models are loaded from previous cells ---\n",
    "if 'pretrained_model' not in locals() or 'finetuned_model' not in locals():\n",
    "    print(\"Models not found in local scope. Please re-run the model loading cells first.\")\n",
    "else:\n",
    "    print(\"--- Replacing Pre-training Heads in Fine-tuned Model's Backbone ---\")\n",
    "\n",
    "    # --- 1. Verify Original Heads (Optional but Recommended) ---\n",
    "    print(\"Original heads BEFORE replacement:\")\n",
    "    # Check object IDs (will be different)\n",
    "    print(f\"  Pretrained dom_head ID: {id(pretrained_model.dom_head)}\")\n",
    "    print(f\"  Finetuned backbone dom_head ID: {id(finetuned_model.backbone.dom_head)}\")\n",
    "    # Check if parameters are currently different (we know they are from previous test)\n",
    "    weights_originally_equal = torch.equal(pretrained_model.dom_head.weight.data, finetuned_model.backbone.dom_head.weight.data)\n",
    "    print(f\"  DOM Head weights originally equal: {weights_originally_equal}\")\n",
    "\n",
    "    # --- 2. Perform the Replacement ---\n",
    "    # Make the dom_head attribute in the fine-tuned backbone point to the\n",
    "    # exact same nn.Linear layer object as in the pre-trained model.\n",
    "    print(\"\\nAssigning pre-trained heads to fine-tuned model's backbone...\")\n",
    "    try:\n",
    "        # Check if the attributes exist before assignment\n",
    "        if hasattr(finetuned_model.backbone, 'dom_head') and hasattr(pretrained_model, 'dom_head'):\n",
    "             finetuned_model.backbone.dom_head = pretrained_model.dom_head\n",
    "             print(\"  - Assigned pretrained dom_head.\")\n",
    "        else:\n",
    "             print(\"  - Could not assign dom_head (missing in source or destination).\")\n",
    "\n",
    "        if hasattr(finetuned_model.backbone, 'charge_head') and hasattr(pretrained_model, 'charge_head'):\n",
    "             finetuned_model.backbone.charge_head = pretrained_model.charge_head\n",
    "             print(\"  - Assigned pretrained charge_head.\")\n",
    "        else:\n",
    "             print(\"  - Could not assign charge_head (missing in source or destination).\")\n",
    "        print(\"Assignment complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during head replacement: {e}\")\n",
    "\n",
    "    # --- 3. Verify Replacement ---\n",
    "    print(\"\\nVerifying heads AFTER replacement:\")\n",
    "    # Check object IDs (should now be the same for dom_head)\n",
    "    print(f\"  Pretrained dom_head ID: {id(pretrained_model.dom_head)}\")\n",
    "    print(f\"  Finetuned backbone dom_head ID (should match above): {id(finetuned_model.backbone.dom_head)}\")\n",
    "\n",
    "    # Check if parameters are now the same (should be True)\n",
    "    try:\n",
    "        weights_now_equal = torch.equal(pretrained_model.dom_head.weight.data, finetuned_model.backbone.dom_head.weight.data)\n",
    "        biases_now_equal = torch.equal(pretrained_model.dom_head.bias.data, finetuned_model.backbone.dom_head.bias.data)\n",
    "        print(f\"  DOM Head weights now equal: {weights_now_equal}\")\n",
    "        print(f\"  DOM Head biases now equal:  {biases_now_equal}\")\n",
    "        if weights_now_equal and biases_now_equal:\n",
    "            print(\"  Verification successful: Fine-tuned backbone now uses identical pre-trained DOM head object.\")\n",
    "        else:\n",
    "            print(\"  Verification FAILED: Parameters are still different after assignment?!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during verification: {e}\")\n",
    "\n",
    "\n",
    "    # --- 4. Re-evaluate DOM Loss (Optional) ---\n",
    "    # You can now re-run the DOM loss calculation on the modified finetuned_model\n",
    "    # to see if the loss is now closer to the pretrained model's loss.\n",
    "    # Make sure the dataloader is ready (might need re-instantiation)\n",
    "    print(\"\\n(Optional) Re-evaluating DOM loss on modified fine-tuned model...\")\n",
    "    # Example: Assuming calculate_dom_loss function and val_loader exist\n",
    "    # val_loader_re = DataLoader(val_dataset, ...) # Recreate dataloader if needed\n",
    "    # avg_dom_loss_finetuned_modified = calculate_dom_loss(finetuned_model, val_loader_re, device)\n",
    "    # print(f\"Pre-trained Model DOM Loss        : {avg_dom_loss_pretrained:.6f}\") # From previous run\n",
    "    # print(f\"Modified Fine-tuned Model DOM Loss: {avg_dom_loss_finetuned_modified:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea68aca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using lambda_charge: 1.0\n",
      "\n",
      "Setting up PRE-TRAINING validation dataloader...\n",
      "Directory: /groups/pheno/inar/icecube_kaggle/memmaped_eval_1M_127\n",
      "Event limit: 100000\n",
      "Batch size: 1024\n",
      "Pre-training validation dataloader created (96 batches).\n",
      "\n",
      "Calculating Pre-training losses for: Pre-trained Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d66b406be304b4dbb308b5d94d15faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval Pretrain Loss (Pre-trained Model):   0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/hpc/pheno/inar/mambaforge/envs/polarbert/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:70: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1728945388353/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return torch.as_tensor(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete for Pre-trained Model. Batches processed for combined loss: 96\n",
      "\n",
      "Calculating Pre-training losses for: Fine-tuned Model (Swapped Heads)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172f8fe2bf164f22a75b1af945dd9398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval Pretrain Loss (Fine-tuned Model (Swapped Heads)):   0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/hpc/pheno/inar/mambaforge/envs/polarbert/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:70: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1728945388353/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return torch.as_tensor(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete for Fine-tuned Model (Swapped Heads). Batches processed for combined loss: 97\n",
      "\n",
      "--- Pre-training Loss Benchmark ---\n",
      "Validation Dataset: /groups/pheno/inar/icecube_kaggle/memmaped_eval_1M_127 (100000 events)\n",
      "Metric          | Pre-trained Model | Fine-tuned (Swapped Heads)\n",
      "----------------|-------------------|---------------------------\n",
      "DOM Loss        | 2.084382          | 4.000487                 \n",
      "Charge Loss     | 0.000578          | 0.585109                 \n",
      "Combined Loss* | 2.084960          | 4.585596                 \n",
      "*Combined = DOM + 1.00 * Charge\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm # Use notebook version of tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# --- Make sure project imports are available ---\n",
    "try:\n",
    "    # Need config, base model, finetuner model structure (to check instance type)\n",
    "    from polarbert.config import PolarBertConfig\n",
    "    from polarbert.time_embed_polarbert import PolarBertModel\n",
    "    from polarbert.te_finetuning import PolarBertFinetuner # Only needed for isinstance check\n",
    "\n",
    "    # Need dataset and default transforms used for pre-training validation\n",
    "    from polarbert.icecube_dataset import IceCubeDataset # Assuming Kaggle type uses this\n",
    "    from polarbert.te_pretraining import default_transform, default_target_transform\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing dataset/transform modules: {e}\")\n",
    "    print(\"Please ensure they are accessible.\")\n",
    "    raise e\n",
    "\n",
    "# --- Ensure Models Exist from Previous Steps ---\n",
    "if 'pretrained_model' not in locals():\n",
    "    print(\"ERROR: 'pretrained_model' not found. Please load the pre-trained model first.\")\n",
    "    raise NameError(\"pretrained_model not defined\")\n",
    "if 'finetuned_model' not in locals():\n",
    "    print(\"ERROR: 'finetuned_model' not found. Please load the fine-tuned model first.\")\n",
    "    raise NameError(\"finetuned_model not defined\")\n",
    "if not hasattr(finetuned_model.backbone, 'dom_head') or id(finetuned_model.backbone.dom_head) != id(pretrained_model.dom_head):\n",
    "     print(\"WARNING: It looks like the dom_head in the fine-tuned model hasn't been replaced\")\n",
    "     print(\"         with the pre-trained one yet. Run the replacement cell first for a fair comparison.\")\n",
    "     # Optionally, perform the replacement here again if desired:\n",
    "     # print(\"Performing head replacement now...\")\n",
    "     # finetuned_model.backbone.dom_head = pretrained_model.dom_head\n",
    "     # finetuned_model.backbone.charge_head = pretrained_model.charge_head\n",
    "\n",
    "# --- 1. Evaluation Setup ---\n",
    "# Use pre-training validation data\n",
    "val_data_dir = '/groups/pheno/inar/icecube_kaggle/memmaped_eval_1M_127'\n",
    "dataset_type = 'kaggle'\n",
    "eval_batch_size = 1024\n",
    "val_events_limit = 100000 # Limit validation events\n",
    "\n",
    "# Other settings\n",
    "num_workers = 1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Retrieve lambda_charge (should be same in both configs ideally)\n",
    "# Load pre-trained config again if not available\n",
    "if 'pretrained_config' not in locals():\n",
    "     print(\"Loading pre-trained config to get lambda_charge...\")\n",
    "     pretrained_config = PolarBertConfig.from_checkpoint(pretrained_ckpt_path) # Need path variable\n",
    "lambda_charge = pretrained_config.model.lambda_charge\n",
    "print(f\"Using lambda_charge: {lambda_charge}\")\n",
    "\n",
    "\n",
    "# --- 2. Create Pre-training Validation Dataloader ---\n",
    "print(f\"\\nSetting up PRE-TRAINING validation dataloader...\")\n",
    "print(f\"Directory: {val_data_dir}\")\n",
    "print(f\"Event limit: {val_events_limit}\")\n",
    "print(f\"Batch size: {eval_batch_size}\")\n",
    "\n",
    "try:\n",
    "    # Use the default target transform for pre-training data\n",
    "    pretrain_val_dataset_full = IceCubeDataset(\n",
    "        data_dir=val_data_dir,\n",
    "        batch_size=eval_batch_size,\n",
    "        transform=default_transform,\n",
    "        target_transform=default_target_transform # Returns numpy y, c\n",
    "    )\n",
    "    pretrain_val_dataset = pretrain_val_dataset_full.slice(0, val_events_limit)\n",
    "    pretrain_val_loader = DataLoader(\n",
    "        pretrain_val_dataset, batch_size=None, num_workers=num_workers,\n",
    "        pin_memory=False, persistent_workers=(num_workers > 0)\n",
    "    )\n",
    "    num_batches = len(pretrain_val_dataset)\n",
    "    print(f\"Pre-training validation dataloader created ({num_batches} batches).\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating pre-training validation dataloader: {e}\")\n",
    "    raise e\n",
    "\n",
    "# --- 3. Define Pre-training Loss Evaluation Function ---\n",
    "\n",
    "def calculate_pretrain_losses(model: torch.nn.Module, dataloader: DataLoader, device: torch.device, lambda_charge: float) -> dict:\n",
    "    \"\"\"Calculates average pre-training losses (DOM, Charge, Combined).\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    total_dom_loss = 0.0\n",
    "    total_charge_loss = 0.0\n",
    "    total_combined_loss = 0.0\n",
    "    batches_with_dom_loss = 0\n",
    "    batches_with_charge_loss = 0\n",
    "    processed_batches_combined = 0\n",
    "\n",
    "    # Determine how to access components\n",
    "    is_finetuner = isinstance(model, PolarBertFinetuner)\n",
    "    if is_finetuner:\n",
    "        embedding_layer = model.backbone.embedding\n",
    "        transformer_blocks = model.backbone.transformer_blocks\n",
    "        final_norm = model.backbone.final_norm\n",
    "        # Access the potentially swapped heads within the backbone\n",
    "        dom_head = model.backbone.dom_head\n",
    "        charge_head = model.backbone.charge_head\n",
    "        model_description = \"Fine-tuned Model (Swapped Heads)\"\n",
    "    elif isinstance(model, PolarBertModel):\n",
    "        embedding_layer = model.embedding\n",
    "        transformer_blocks = model.transformer_blocks\n",
    "        final_norm = model.final_norm\n",
    "        dom_head = model.dom_head\n",
    "        charge_head = model.charge_head\n",
    "        model_description = \"Pre-trained Model\"\n",
    "    else:\n",
    "        raise TypeError(\"Model type not recognized (expected PolarBertModel or PolarBertFinetuner)\")\n",
    "\n",
    "    print(f\"\\nCalculating Pre-training losses for: {model_description}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, total=num_batches, desc=f\"Eval Pretrain Loss ({model_description})\")\n",
    "        for batch in pbar:\n",
    "            if batch is None: continue\n",
    "            # Pre-training target transform returns numpy arrays y, c\n",
    "            (x, l), y_data = batch\n",
    "            if y_data is None: continue # Skip if no labels\n",
    "\n",
    "            # Extract targets - y_data is tuple (y_numpy, c_numpy)\n",
    "            # y_numpy is ignored here, charge c_numpy is needed\n",
    "            true_total_charge_numpy = y_data[1]\n",
    "\n",
    "            x = x.to(device)\n",
    "            true_dom_ids = x[:, :, 3].long() # Assuming index 3 is sensor_id\n",
    "\n",
    "            # Convert charge target to tensor *before* log10\n",
    "            if true_total_charge_numpy is not None:\n",
    "                 true_total_charge = torch.as_tensor(true_total_charge_numpy, device=device)\n",
    "            else:\n",
    "                 true_total_charge = None\n",
    "\n",
    "            try:\n",
    "                # --- Manual Forward Pass ---\n",
    "                hidden_states, final_padding_mask, output_mask = embedding_layer((x, l))\n",
    "                attn_key_padding_mask = final_padding_mask\n",
    "                for block in transformer_blocks:\n",
    "                    hidden_states = block(hidden_states, key_padding_mask=attn_key_padding_mask)\n",
    "                hidden_states = final_norm(hidden_states)\n",
    "                cls_embed = hidden_states[:, 0, :]\n",
    "                sequence_embeds = hidden_states[:, 1:, :] # Exclude CLS\n",
    "\n",
    "                # --- Predictions ---\n",
    "                dom_logits = dom_head(sequence_embeds)\n",
    "                charge_pred = charge_head(cls_embed)\n",
    "\n",
    "                # --- Calculate Losses ---\n",
    "                # DOM Loss\n",
    "                current_dom_loss = torch.tensor(float('nan'), device=device)\n",
    "                if output_mask is not None and output_mask.sum() > 0:\n",
    "                    dom_targets = true_dom_ids - 1\n",
    "                    masked_logits = dom_logits[output_mask]\n",
    "                    masked_targets = dom_targets[output_mask]\n",
    "                    loss_val_dom = F.cross_entropy(masked_logits, masked_targets.long(), ignore_index=-1)\n",
    "                    if not torch.isnan(loss_val_dom):\n",
    "                        current_dom_loss = loss_val_dom\n",
    "                        total_dom_loss += current_dom_loss.item()\n",
    "                        batches_with_dom_loss += 1\n",
    "                elif processed_batches_combined < 1: # Warn only once\n",
    "                    warnings.warn(\"output_mask is None or empty. Cannot calculate DOM loss.\", RuntimeWarning)\n",
    "\n",
    "                # Charge Loss\n",
    "                current_charge_loss = torch.tensor(float('nan'), device=device)\n",
    "                if true_total_charge is not None:\n",
    "                    true_log_charge = torch.log10(torch.clamp(true_total_charge.float(), min=1e-6))\n",
    "                    loss_val_charge = F.mse_loss(charge_pred.squeeze(-1), true_log_charge)\n",
    "                    if not torch.isnan(loss_val_charge):\n",
    "                        current_charge_loss = loss_val_charge\n",
    "                        total_charge_loss += current_charge_loss.item()\n",
    "                        batches_with_charge_loss += 1\n",
    "\n",
    "                # Combined Loss\n",
    "                dom_val = current_dom_loss.item() if not torch.isnan(current_dom_loss) else 0.0\n",
    "                charge_val = current_charge_loss.item() if not torch.isnan(current_charge_loss) else 0.0\n",
    "                combined_loss = dom_val + lambda_charge * charge_val\n",
    "\n",
    "                if not np.isnan(combined_loss): # Check combined float value\n",
    "                     total_combined_loss += combined_loss\n",
    "                     processed_batches_combined += 1 # Increment only if combined is valid\n",
    "\n",
    "            except Exception as e:\n",
    "                 print(f\"\\nError during forward/loss calculation in batch {processed_batches_combined}: {e}\")\n",
    "                 warnings.warn(f\"Skipping batch {processed_batches_combined} due to error.\", RuntimeWarning)\n",
    "\n",
    "            # Update progress bar\n",
    "            if processed_batches_combined > 0:\n",
    "                 pbar.set_postfix({\n",
    "                     \"Avg DOM\": f\"{total_dom_loss / batches_with_dom_loss:.4f}\" if batches_with_dom_loss > 0 else \"NaN\",\n",
    "                     \"Avg Chrg\": f\"{total_charge_loss / batches_with_charge_loss:.4f}\" if batches_with_charge_loss > 0 else \"NaN\"\n",
    "                 })\n",
    "\n",
    "    # --- Calculate Averages ---\n",
    "    avg_dom = (total_dom_loss / batches_with_dom_loss) if batches_with_dom_loss > 0 else float('nan')\n",
    "    avg_charge = (total_charge_loss / batches_with_charge_loss) if batches_with_charge_loss > 0 else float('nan')\n",
    "    avg_combined = (total_combined_loss / processed_batches_combined) if processed_batches_combined > 0 else float('nan')\n",
    "\n",
    "    print(f\"Evaluation complete for {model_description}. Batches processed for combined loss: {processed_batches_combined}\")\n",
    "\n",
    "    return {\"dom\": avg_dom, \"charge\": avg_charge, \"combined\": avg_combined}\n",
    "\n",
    "# --- 4. Run Evaluation ---\n",
    "results_pretrained = calculate_pretrain_losses(pretrained_model, pretrain_val_loader, device, lambda_charge)\n",
    "\n",
    "# Recreate dataloader for the second model\n",
    "pretrain_val_loader_2 = DataLoader(pretrain_val_dataset, batch_size=None, num_workers=num_workers, pin_memory=False, persistent_workers=(num_workers > 0))\n",
    "# Ensure finetuned_model is the one with swapped heads\n",
    "results_finetuned_swapped = calculate_pretrain_losses(finetuned_model, pretrain_val_loader_2, device, lambda_charge)\n",
    "\n",
    "# --- 5. Print Comparison ---\n",
    "print(\"\\n--- Pre-training Loss Benchmark ---\")\n",
    "print(f\"Validation Dataset: {val_data_dir} ({val_events_limit} events)\")\n",
    "print(f\"Metric          | Pre-trained Model | Fine-tuned (Swapped Heads)\")\n",
    "print(f\"----------------|-------------------|---------------------------\")\n",
    "print(f\"DOM Loss        | {results_pretrained['dom']:<17.6f} | {results_finetuned_swapped['dom']:<25.6f}\")\n",
    "print(f\"Charge Loss     | {results_pretrained['charge']:<17.6f} | {results_finetuned_swapped['charge']:<25.6f}\")\n",
    "print(f\"Combined Loss* | {results_pretrained['combined']:<17.6f} | {results_finetuned_swapped['combined']:<25.6f}\")\n",
    "print(f\"*Combined = DOM + {lambda_charge:.2f} * Charge\")\n",
    "print(\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5719196a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polarbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
