data:
  max_per_device_batch_size: 1024 # Maximum batch size that fits on the GPU
  train_dir: '/path/to/train/data'
  val_dir: '/path/to/val/data'
  train_events: 3000000
  val_events: 100000
  pin_memory: true
  num_workers: 1 # Should remain 1 when using an IterableDataset
  persistent_workers: true

model:
  embedding_dim: 256
  dom_embed_dim: 128
  num_heads: 8
  hidden_size: 1024
  num_layers: 8
  directional:
    hidden_size: 1024

training:
  mask_prob: 0.0 # No masking for finetuning
  max_epochs: 20
  logical_batch_size: 200 # Batch size used for training (will use gradient accumulation if necessary)
  max_lr: 7e-4  # the main lr for the schedule-free option
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_eps: 1e-8
  weight_decay: 0.0
  warm_up_steps: 0  # for schedule-free
  amsgrad: false
  lr_scheduler: 'onecycle'

  pct_start: 0.01 # not used by schedule-free
  div_factor: 25.0  # not used by schedule-free
  final_div_factor: 1e4 # not used by schedule-free
  project: 'PolarBERT-finetuning'
  
  gpus: 1
  precision: "16-mixed"
  gradient_clip_val: 2.0
  val_check_interval: 1.0

  checkpoint:
    dirpath: 'checkpoints'
    save_top_k: 1
    monitor: 'val/loss'
    mode: 'min'
    save_last: true
    save_final: true

pretrained:
  model_type: 'flash'
  checkpoint_path: '/path/to/pretrained/model.pth' # Or 'new' to train from scratch
  freeze_backbone: false
